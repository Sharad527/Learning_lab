{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37540b",
   "metadata": {},
   "source": [
    "# We will solve the Simple Harmonic equation here\n",
    "$$\n",
    "\\frac{d^2 }{dx^2}y(x)+y(x)=0\n",
    "$$\n",
    "for the boundary conditions $y(0)=1$ and $y'(0)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f26662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.6383, 4.5298, 0.4752, 3.4954, 0.6522, 0.0294, 3.8676, 2.8668, 2.7053,\n",
      "        2.4226, 1.9535, 2.0645, 2.4603, 1.6300, 0.8331, 0.6064, 1.4571, 0.4152,\n",
      "        1.4406, 4.5115, 2.3954, 2.8549, 1.4749, 2.3120, 1.1718, 3.4141, 1.8085,\n",
      "        1.0700, 1.9980, 4.5926, 2.2950, 4.1458, 4.3056, 0.8922, 0.0895, 0.4196,\n",
      "        3.9262, 4.0905, 0.3786, 1.0599, 1.4348, 2.0972, 2.6417, 2.4149, 4.1967,\n",
      "        4.7922, 0.7808, 0.4434, 0.8382, 1.6320], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[4.6383],\n",
      "        [4.5298],\n",
      "        [0.4752],\n",
      "        [3.4954],\n",
      "        [0.6522],\n",
      "        [0.0294],\n",
      "        [3.8676],\n",
      "        [2.8668],\n",
      "        [2.7053],\n",
      "        [2.4226],\n",
      "        [1.9535],\n",
      "        [2.0645],\n",
      "        [2.4603],\n",
      "        [1.6300],\n",
      "        [0.8331],\n",
      "        [0.6064],\n",
      "        [1.4571],\n",
      "        [0.4152],\n",
      "        [1.4406],\n",
      "        [4.5115],\n",
      "        [2.3954],\n",
      "        [2.8549],\n",
      "        [1.4749],\n",
      "        [2.3120],\n",
      "        [1.1718],\n",
      "        [3.4141],\n",
      "        [1.8085],\n",
      "        [1.0700],\n",
      "        [1.9980],\n",
      "        [4.5926],\n",
      "        [2.2950],\n",
      "        [4.1458],\n",
      "        [4.3056],\n",
      "        [0.8922],\n",
      "        [0.0895],\n",
      "        [0.4196],\n",
      "        [3.9262],\n",
      "        [4.0905],\n",
      "        [0.3786],\n",
      "        [1.0599],\n",
      "        [1.4348],\n",
      "        [2.0972],\n",
      "        [2.6417],\n",
      "        [2.4149],\n",
      "        [4.1967],\n",
      "        [4.7922],\n",
      "        [0.7808],\n",
      "        [0.4434],\n",
      "        [0.8382],\n",
      "        [1.6320]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LEts start by defining the domain in which I want the solution of this differntial equation\n",
    "random_array = np.random.uniform(low = 0, high = 5, size = 50)\n",
    "x = torch.tensor(random_array,requires_grad=True)\n",
    "x = x.float()\n",
    "print(x)\n",
    "x_input = x.unsqueeze(dim=1)\n",
    "\n",
    "x_input.requires_grad_(True)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de35b4",
   "metadata": {},
   "source": [
    "## Now I will define neural network and everything else will follow accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0fbdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, in_layer = int, out_layer = int, num_of_layer = int, hidden_size = int):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_layer, hidden_size)\n",
    "        self.activation = nn.Tanh() # For smooth derivatives\n",
    "\n",
    "        # Creating scalable model\n",
    "        hidden_layer = []\n",
    "        for _ in range(num_of_layer-1):\n",
    "            hidden_layer.append(nn.Linear(in_features = hidden_size, out_features = hidden_size))\n",
    "            hidden_layer.append(nn.Tanh())\n",
    "        \n",
    "        # Registering the list of layers as module list\n",
    "        self.hidden_stack = nn.ModuleList(hidden_layer)\n",
    "\n",
    "\n",
    "        #Defining the output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, out_layer)\n",
    "    \n",
    "\n",
    "    #Forward pass\n",
    "    def forward(self, x: torch.tensor):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "\n",
    "        for layer in self.hidden_stack:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "# 2. Define the Initialization Rule\n",
    "def xavier_init_rule(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51972d7f",
   "metadata": {},
   "source": [
    "# Defining a new instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4989ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1391],\n",
      "        [0.1385],\n",
      "        [0.0451],\n",
      "        [0.1306],\n",
      "        [0.0577],\n",
      "        [0.0031],\n",
      "        [0.1340],\n",
      "        [0.1231],\n",
      "        [0.1208],\n",
      "        [0.1161],\n",
      "        [0.1065],\n",
      "        [0.1090],\n",
      "        [0.1167],\n",
      "        [0.0984],\n",
      "        [0.0683],\n",
      "        [0.0547],\n",
      "        [0.0933],\n",
      "        [0.0403],\n",
      "        [0.0928],\n",
      "        [0.1384],\n",
      "        [0.1156],\n",
      "        [0.1230],\n",
      "        [0.0938],\n",
      "        [0.1140],\n",
      "        [0.0835],\n",
      "        [0.1298],\n",
      "        [0.1031],\n",
      "        [0.0794],\n",
      "        [0.1076],\n",
      "        [0.1389],\n",
      "        [0.1137],\n",
      "        [0.1361],\n",
      "        [0.1372],\n",
      "        [0.0713],\n",
      "        [0.0094],\n",
      "        [0.0406],\n",
      "        [0.1345],\n",
      "        [0.1357],\n",
      "        [0.0372],\n",
      "        [0.0790],\n",
      "        [0.0926],\n",
      "        [0.1097],\n",
      "        [0.1198],\n",
      "        [0.1159],\n",
      "        [0.1364],\n",
      "        [0.1399],\n",
      "        [0.0654],\n",
      "        [0.0426],\n",
      "        [0.0685],\n",
      "        [0.0984]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's see if the code is working\n",
    "# torch.manual_seed(10)\n",
    "\n",
    "# Creating an instance of our neural network\n",
    "my_first_model = PINN(1,1,5,5)\n",
    "\n",
    "my_first_model.apply(xavier_init_rule)\n",
    "\n",
    "\n",
    "y_preds = my_first_model(x_input)\n",
    "print(y_preds)\n",
    "\n",
    "# my_first_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4db168",
   "metadata": {},
   "source": [
    "# Boundary condition\n",
    "Now at this point I am only left with one last ingredient to create the loss function. The boundary conditions. Since, boundary condition of the form\n",
    "$$\n",
    "\\mathcal{L}_{BC} = ||y_{pred}|_{x=0}-y_{constant}|_{x=0}||^2\n",
    "$$\n",
    "This means I need another tensor which is the point x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62afdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the boundary point\n",
    "x_bc = torch.tensor(0.0, requires_grad = True).unsqueeze(dim=-1)\n",
    "\n",
    "# Similarly I need to setup the y point which has a constant value\n",
    "y_bc = torch.tensor(1.0, requires_grad = False)\n",
    "\n",
    "# I understand that I need to output the value of network at the pont x_bc which should always be equal to y_bc\n",
    "\n",
    "# Also I need to specify dy_dx_bc to impose the second boundary condition\n",
    "dy_dx_bc = torch.tensor(1.0, requires_grad=False)\n",
    "\n",
    "# y_pred_bc = my_first_model(x_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ced76",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "I have to choose an optimiser as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004a9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=my_first_model.parameters(),lr=0.0001, weight_decay= 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c08df2",
   "metadata": {},
   "source": [
    "# ORder of a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ae9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order(number):\n",
    "    if number ==0:\n",
    "        return print(\"order not defined\")\n",
    "    else :\n",
    "        order_abs = np.floor(np.log10(abs(number)))\n",
    "    return order_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57662a3",
   "metadata": {},
   "source": [
    "# Obtainng the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea91742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_learning_rate(order):\n",
    "    return 10**(order-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b135d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(order(.350))\n",
    "print(lower_learning_rate(order(035.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71e26c",
   "metadata": {},
   "source": [
    "# Time to train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96de69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  | Total loss: [1.8892851] | Learning rate: 0.009999999776482582\n",
      "Epoch: 100  | Total loss: [0.91109324] | Learning rate: 0.0010000000474974513\n",
      "Epoch: 200  | Total loss: [0.87796474] | Learning rate: 0.0010000000474974513\n",
      "Epoch: 300  | Total loss: [0.8601246] | Learning rate: 0.0010000000474974513\n",
      "Epoch: 400  | Total loss: [0.832867] | Learning rate: 0.0010000000474974513\n",
      "Epoch: 500  | Total loss: [0.7707187] | Learning rate: 0.0010000000474974513\n",
      "Epoch: 600  | Total loss: [0.10003427] | Learning rate: 0.0010000000474974513\n",
      "Epoch: 700  | Total loss: [0.05557681] | Learning rate: 9.999999747378752e-05\n",
      "Epoch: 800  | Total loss: [0.03683791] | Learning rate: 9.999999747378752e-05\n",
      "Epoch: 900  | Total loss: [0.02525128] | Learning rate: 9.999999747378752e-05\n",
      "Epoch: 1000  | Total loss: [0.01759593] | Learning rate: 9.999999747378752e-05\n",
      "Epoch: 1100  | Total loss: [0.0123349] | Learning rate: 9.999999747378752e-05\n",
      "Epoch: 1200  | Total loss: [0.00984744] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1300  | Total loss: [0.00950078] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1400  | Total loss: [0.00915169] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1500  | Total loss: [0.00880187] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1600  | Total loss: [0.00845269] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1700  | Total loss: [0.00810594] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1800  | Total loss: [0.00776253] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 1900  | Total loss: [0.00742391] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2000  | Total loss: [0.00709088] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2100  | Total loss: [0.00676461] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2200  | Total loss: [0.0064458] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2300  | Total loss: [0.00613536] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2400  | Total loss: [0.00583411] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2500  | Total loss: [0.0055424] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2600  | Total loss: [0.00526113] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2700  | Total loss: [0.00499044] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2800  | Total loss: [0.00473089] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 2900  | Total loss: [0.00448292] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3000  | Total loss: [0.00424661] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3100  | Total loss: [0.0040222] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3200  | Total loss: [0.00380965] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3300  | Total loss: [0.00360912] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3400  | Total loss: [0.00342048] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3500  | Total loss: [0.00324367] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3600  | Total loss: [0.00307857] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3700  | Total loss: [0.00292475] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3800  | Total loss: [0.00278215] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 3900  | Total loss: [0.0026503] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4000  | Total loss: [0.00252882] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4100  | Total loss: [0.00241722] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4200  | Total loss: [0.00231503] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4300  | Total loss: [0.00222175] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4400  | Total loss: [0.00213676] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4500  | Total loss: [0.00205962] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4600  | Total loss: [0.00198957] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4700  | Total loss: [0.00192606] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4800  | Total loss: [0.00186854] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 4900  | Total loss: [0.00181647] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5000  | Total loss: [0.00176913] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5100  | Total loss: [0.00172604] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5200  | Total loss: [0.00168674] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5300  | Total loss: [0.0016507] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5400  | Total loss: [0.00161758] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5500  | Total loss: [0.0015868] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5600  | Total loss: [0.00155811] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5700  | Total loss: [0.00153118] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5800  | Total loss: [0.00150577] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 5900  | Total loss: [0.00148159] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6000  | Total loss: [0.00145847] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6100  | Total loss: [0.00143627] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6200  | Total loss: [0.00141477] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6300  | Total loss: [0.00139382] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6400  | Total loss: [0.00137341] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6500  | Total loss: [0.00135343] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6600  | Total loss: [0.00133371] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6700  | Total loss: [0.00131426] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6800  | Total loss: [0.00129507] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 6900  | Total loss: [0.00127595] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7000  | Total loss: [0.00125697] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7100  | Total loss: [0.00123804] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7200  | Total loss: [0.00121916] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7300  | Total loss: [0.00120029] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7400  | Total loss: [0.00118138] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7500  | Total loss: [0.00116251] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7600  | Total loss: [0.00114351] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7700  | Total loss: [0.00112457] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7800  | Total loss: [0.00110552] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 7900  | Total loss: [0.00108643] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 8000  | Total loss: [0.00106727] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 8100  | Total loss: [0.00104807] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 8200  | Total loss: [0.00102888] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 8300  | Total loss: [0.00100961] | Learning rate: 9.999999747378752e-06\n",
      "Epoch: 8400  | Total loss: [0.00099896] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 8500  | Total loss: [0.00099699] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 8600  | Total loss: [0.00099499] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 8700  | Total loss: [0.00099291] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 8800  | Total loss: [0.00099079] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 8900  | Total loss: [0.00098857] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9000  | Total loss: [0.00098635] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9100  | Total loss: [0.00098407] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9200  | Total loss: [0.00098174] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9300  | Total loss: [0.00097933] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9400  | Total loss: [0.00097696] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9500  | Total loss: [0.00097452] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9600  | Total loss: [0.00097203] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9700  | Total loss: [0.00096945] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9800  | Total loss: [0.00096685] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 9900  | Total loss: [0.00096429] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10000  | Total loss: [0.00096164] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10100  | Total loss: [0.000959] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10200  | Total loss: [0.00095627] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10300  | Total loss: [0.00095353] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10400  | Total loss: [0.00095069] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10500  | Total loss: [0.00094786] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10600  | Total loss: [0.00094501] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10700  | Total loss: [0.00094213] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10800  | Total loss: [0.00093921] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 10900  | Total loss: [0.00093627] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11000  | Total loss: [0.00093331] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11100  | Total loss: [0.00093037] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11200  | Total loss: [0.00092743] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11300  | Total loss: [0.00092438] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11400  | Total loss: [0.00092134] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11500  | Total loss: [0.0009183] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11600  | Total loss: [0.00091521] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11700  | Total loss: [0.00091211] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11800  | Total loss: [0.00090909] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 11900  | Total loss: [0.00090594] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12000  | Total loss: [0.00090286] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12100  | Total loss: [0.00089976] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12200  | Total loss: [0.00089663] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12300  | Total loss: [0.00089347] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12400  | Total loss: [0.00089028] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12500  | Total loss: [0.00088714] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12600  | Total loss: [0.00088402] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12700  | Total loss: [0.00088086] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12800  | Total loss: [0.00087767] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 12900  | Total loss: [0.00087449] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13000  | Total loss: [0.00087133] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13100  | Total loss: [0.00086818] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13200  | Total loss: [0.00086503] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13300  | Total loss: [0.00086182] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13400  | Total loss: [0.00085865] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13500  | Total loss: [0.00085547] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13600  | Total loss: [0.00085229] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13700  | Total loss: [0.00084916] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13800  | Total loss: [0.00084594] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 13900  | Total loss: [0.00084282] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14000  | Total loss: [0.00083963] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14100  | Total loss: [0.00083647] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14200  | Total loss: [0.00083331] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14300  | Total loss: [0.00083019] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14400  | Total loss: [0.00082706] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14500  | Total loss: [0.00082388] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14600  | Total loss: [0.00082073] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14700  | Total loss: [0.00081761] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14800  | Total loss: [0.00081448] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 14900  | Total loss: [0.00081141] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15000  | Total loss: [0.00080831] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15100  | Total loss: [0.00080521] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15200  | Total loss: [0.00080212] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15300  | Total loss: [0.00079907] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15400  | Total loss: [0.00079602] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15500  | Total loss: [0.00079295] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15600  | Total loss: [0.00078995] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15700  | Total loss: [0.00078687] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15800  | Total loss: [0.00078383] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 15900  | Total loss: [0.00078081] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16000  | Total loss: [0.00077776] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16100  | Total loss: [0.00077476] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16200  | Total loss: [0.00077174] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16300  | Total loss: [0.00076876] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16400  | Total loss: [0.00076581] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16500  | Total loss: [0.0007628] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16600  | Total loss: [0.00075984] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16700  | Total loss: [0.00075688] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16800  | Total loss: [0.00075394] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 16900  | Total loss: [0.00075099] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17000  | Total loss: [0.00074808] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17100  | Total loss: [0.00074509] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17200  | Total loss: [0.00074218] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17300  | Total loss: [0.00073927] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17400  | Total loss: [0.00073637] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17500  | Total loss: [0.00073348] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17600  | Total loss: [0.00073062] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17700  | Total loss: [0.0007277] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17800  | Total loss: [0.00072486] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 17900  | Total loss: [0.00072196] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18000  | Total loss: [0.00071909] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18100  | Total loss: [0.00071623] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18200  | Total loss: [0.00071338] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18300  | Total loss: [0.00071057] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18400  | Total loss: [0.00070772] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18500  | Total loss: [0.00070489] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18600  | Total loss: [0.00070207] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18700  | Total loss: [0.00069928] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18800  | Total loss: [0.00069645] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 18900  | Total loss: [0.00069366] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19000  | Total loss: [0.00069085] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19100  | Total loss: [0.00068811] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19200  | Total loss: [0.0006853] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19300  | Total loss: [0.00068252] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19400  | Total loss: [0.00067975] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19500  | Total loss: [0.00067697] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19600  | Total loss: [0.00067422] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19700  | Total loss: [0.00067146] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19800  | Total loss: [0.00066872] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 19900  | Total loss: [0.00066597] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20000  | Total loss: [0.00066323] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20100  | Total loss: [0.00066052] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20200  | Total loss: [0.00065777] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20300  | Total loss: [0.00065507] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20400  | Total loss: [0.00065237] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20500  | Total loss: [0.00064968] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20600  | Total loss: [0.00064698] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20700  | Total loss: [0.00064429] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20800  | Total loss: [0.00064163] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 20900  | Total loss: [0.00063893] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21000  | Total loss: [0.00063628] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21100  | Total loss: [0.00063359] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21200  | Total loss: [0.00063093] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21300  | Total loss: [0.00062823] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21400  | Total loss: [0.00062561] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21500  | Total loss: [0.00062295] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21600  | Total loss: [0.00062028] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21700  | Total loss: [0.00061764] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21800  | Total loss: [0.00061502] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 21900  | Total loss: [0.00061238] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22000  | Total loss: [0.00060974] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22100  | Total loss: [0.00060708] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22200  | Total loss: [0.00060448] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22300  | Total loss: [0.00060184] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22400  | Total loss: [0.00059923] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22500  | Total loss: [0.00059662] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22600  | Total loss: [0.00059401] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22700  | Total loss: [0.00059143] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22800  | Total loss: [0.00058881] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 22900  | Total loss: [0.00058629] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23000  | Total loss: [0.00058367] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23100  | Total loss: [0.00058105] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23200  | Total loss: [0.00057854] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23300  | Total loss: [0.00057597] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23400  | Total loss: [0.00057338] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23500  | Total loss: [0.00057084] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23600  | Total loss: [0.00056828] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23700  | Total loss: [0.00056575] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23800  | Total loss: [0.0005632] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 23900  | Total loss: [0.00056067] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24000  | Total loss: [0.00055812] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24100  | Total loss: [0.00055564] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24200  | Total loss: [0.0005531] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24300  | Total loss: [0.00055056] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24400  | Total loss: [0.00054807] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24500  | Total loss: [0.00054555] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24600  | Total loss: [0.00054308] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24700  | Total loss: [0.00054056] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24800  | Total loss: [0.00053804] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 24900  | Total loss: [0.00053561] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25000  | Total loss: [0.00053313] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25100  | Total loss: [0.00053064] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25200  | Total loss: [0.00052816] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25300  | Total loss: [0.0005257] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25400  | Total loss: [0.00052324] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25500  | Total loss: [0.00052077] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25600  | Total loss: [0.00051835] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25700  | Total loss: [0.00051586] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25800  | Total loss: [0.0005134] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 25900  | Total loss: [0.00051102] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26000  | Total loss: [0.00050855] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26100  | Total loss: [0.0005061] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26200  | Total loss: [0.0005037] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26300  | Total loss: [0.0005013] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26400  | Total loss: [0.00049889] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26500  | Total loss: [0.00049644] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26600  | Total loss: [0.00049407] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26700  | Total loss: [0.00049167] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26800  | Total loss: [0.00048926] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 26900  | Total loss: [0.00048688] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27000  | Total loss: [0.00048448] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27100  | Total loss: [0.00048209] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27200  | Total loss: [0.00047973] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27300  | Total loss: [0.00047736] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27400  | Total loss: [0.000475] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27500  | Total loss: [0.00047263] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27600  | Total loss: [0.00047029] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27700  | Total loss: [0.00046792] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27800  | Total loss: [0.00046557] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 27900  | Total loss: [0.00046323] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28000  | Total loss: [0.00046089] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28100  | Total loss: [0.00045856] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28200  | Total loss: [0.00045625] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28300  | Total loss: [0.00045393] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28400  | Total loss: [0.00045161] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28500  | Total loss: [0.00044931] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28600  | Total loss: [0.00044702] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28700  | Total loss: [0.0004447] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28800  | Total loss: [0.00044241] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 28900  | Total loss: [0.00044012] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29000  | Total loss: [0.00043785] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29100  | Total loss: [0.00043557] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29200  | Total loss: [0.00043331] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29300  | Total loss: [0.00043104] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29400  | Total loss: [0.00042878] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29500  | Total loss: [0.00042652] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29600  | Total loss: [0.0004243] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29700  | Total loss: [0.00042204] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29800  | Total loss: [0.00041979] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 29900  | Total loss: [0.00041758] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30000  | Total loss: [0.00041535] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30100  | Total loss: [0.00041312] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30200  | Total loss: [0.00041088] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30300  | Total loss: [0.0004087] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30400  | Total loss: [0.00040651] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30500  | Total loss: [0.00040429] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30600  | Total loss: [0.00040211] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30700  | Total loss: [0.00039994] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30800  | Total loss: [0.00039775] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 30900  | Total loss: [0.0003956] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31000  | Total loss: [0.0003934] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31100  | Total loss: [0.00039125] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31200  | Total loss: [0.00038909] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31300  | Total loss: [0.00038695] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31400  | Total loss: [0.00038481] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31500  | Total loss: [0.00038266] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31600  | Total loss: [0.00038052] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31700  | Total loss: [0.00037841] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31800  | Total loss: [0.00037628] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 31900  | Total loss: [0.00037415] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32000  | Total loss: [0.00037206] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32100  | Total loss: [0.00036996] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32200  | Total loss: [0.00036783] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32300  | Total loss: [0.00036573] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32400  | Total loss: [0.00036363] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32500  | Total loss: [0.00036156] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32600  | Total loss: [0.00035948] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32700  | Total loss: [0.00035741] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32800  | Total loss: [0.00035535] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 32900  | Total loss: [0.00035328] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33000  | Total loss: [0.0003512] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33100  | Total loss: [0.00034918] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33200  | Total loss: [0.00034711] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33300  | Total loss: [0.00034506] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33400  | Total loss: [0.00034307] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33500  | Total loss: [0.00034103] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33600  | Total loss: [0.00033901] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33700  | Total loss: [0.000337] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33800  | Total loss: [0.000335] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 33900  | Total loss: [0.00033299] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34000  | Total loss: [0.00033099] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34100  | Total loss: [0.00032899] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34200  | Total loss: [0.00032701] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34300  | Total loss: [0.00032505] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34400  | Total loss: [0.00032306] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34500  | Total loss: [0.00032108] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34600  | Total loss: [0.00031914] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34700  | Total loss: [0.00031716] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34800  | Total loss: [0.00031525] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 34900  | Total loss: [0.00031331] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35000  | Total loss: [0.00031136] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35100  | Total loss: [0.00030943] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35200  | Total loss: [0.00030749] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35300  | Total loss: [0.00030559] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35400  | Total loss: [0.00030367] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35500  | Total loss: [0.00030177] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35600  | Total loss: [0.00029987] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35700  | Total loss: [0.00029801] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35800  | Total loss: [0.00029611] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 35900  | Total loss: [0.00029423] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36000  | Total loss: [0.00029237] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36100  | Total loss: [0.00029049] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36200  | Total loss: [0.00028863] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36300  | Total loss: [0.00028679] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36400  | Total loss: [0.00028496] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36500  | Total loss: [0.00028311] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36600  | Total loss: [0.00028127] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36700  | Total loss: [0.00027945] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36800  | Total loss: [0.00027762] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 36900  | Total loss: [0.00027583] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37000  | Total loss: [0.000274] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37100  | Total loss: [0.00027223] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37200  | Total loss: [0.00027043] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37300  | Total loss: [0.00026865] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37400  | Total loss: [0.00026687] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37500  | Total loss: [0.00026511] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37600  | Total loss: [0.00026336] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37700  | Total loss: [0.0002616] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37800  | Total loss: [0.00025987] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 37900  | Total loss: [0.00025811] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38000  | Total loss: [0.00025639] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38100  | Total loss: [0.00025465] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38200  | Total loss: [0.00025293] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38300  | Total loss: [0.00025125] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38400  | Total loss: [0.00024954] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38500  | Total loss: [0.00024783] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38600  | Total loss: [0.00024615] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38700  | Total loss: [0.00024448] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38800  | Total loss: [0.00024281] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 38900  | Total loss: [0.00024111] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39000  | Total loss: [0.00023945] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39100  | Total loss: [0.0002378] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39200  | Total loss: [0.00023616] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39300  | Total loss: [0.00023455] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39400  | Total loss: [0.00023292] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39500  | Total loss: [0.0002313] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39600  | Total loss: [0.00022968] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39700  | Total loss: [0.00022806] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39800  | Total loss: [0.00022647] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 39900  | Total loss: [0.00022487] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40000  | Total loss: [0.00022328] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40100  | Total loss: [0.00022172] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40200  | Total loss: [0.00022015] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40300  | Total loss: [0.00021857] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40400  | Total loss: [0.00021701] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40500  | Total loss: [0.00021546] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40600  | Total loss: [0.00021394] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40700  | Total loss: [0.0002124] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40800  | Total loss: [0.00021088] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 40900  | Total loss: [0.00020935] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41000  | Total loss: [0.00020785] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41100  | Total loss: [0.00020634] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41200  | Total loss: [0.00020486] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41300  | Total loss: [0.00020339] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41400  | Total loss: [0.0002019] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41500  | Total loss: [0.00020043] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41600  | Total loss: [0.00019896] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41700  | Total loss: [0.0001975] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41800  | Total loss: [0.00019604] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 41900  | Total loss: [0.00019463] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42000  | Total loss: [0.00019319] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42100  | Total loss: [0.00019178] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42200  | Total loss: [0.00019036] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42300  | Total loss: [0.00018895] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42400  | Total loss: [0.00018757] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42500  | Total loss: [0.00018617] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42600  | Total loss: [0.00018478] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42700  | Total loss: [0.00018341] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42800  | Total loss: [0.00018201] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 42900  | Total loss: [0.00018067] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43000  | Total loss: [0.00017932] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43100  | Total loss: [0.00017797] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43200  | Total loss: [0.00017663] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43300  | Total loss: [0.0001753] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43400  | Total loss: [0.00017399] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43500  | Total loss: [0.00017269] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43600  | Total loss: [0.00017138] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43700  | Total loss: [0.00017009] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43800  | Total loss: [0.00016879] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 43900  | Total loss: [0.00016752] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44000  | Total loss: [0.00016624] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44100  | Total loss: [0.00016497] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44200  | Total loss: [0.00016372] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44300  | Total loss: [0.00016247] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44400  | Total loss: [0.00016123] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44500  | Total loss: [0.00015999] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44600  | Total loss: [0.00015877] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44700  | Total loss: [0.00015757] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44800  | Total loss: [0.00015637] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 44900  | Total loss: [0.00015517] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45000  | Total loss: [0.00015396] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45100  | Total loss: [0.00015278] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45200  | Total loss: [0.00015162] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45300  | Total loss: [0.00015045] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45400  | Total loss: [0.00014928] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45500  | Total loss: [0.00014814] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45600  | Total loss: [0.000147] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45700  | Total loss: [0.00014587] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45800  | Total loss: [0.00014474] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 45900  | Total loss: [0.00014363] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46000  | Total loss: [0.00014252] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46100  | Total loss: [0.00014142] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46200  | Total loss: [0.00014032] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46300  | Total loss: [0.00013924] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46400  | Total loss: [0.00013816] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46500  | Total loss: [0.00013709] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46600  | Total loss: [0.00013604] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46700  | Total loss: [0.00013499] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46800  | Total loss: [0.00013393] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 46900  | Total loss: [0.00013289] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47000  | Total loss: [0.00013186] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47100  | Total loss: [0.00013085] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47200  | Total loss: [0.00012982] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47300  | Total loss: [0.00012883] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47400  | Total loss: [0.00012781] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47500  | Total loss: [0.00012679] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47600  | Total loss: [0.00012583] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47700  | Total loss: [0.00012484] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47800  | Total loss: [0.00012386] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 47900  | Total loss: [0.0001229] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48000  | Total loss: [0.00012194] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48100  | Total loss: [0.000121] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48200  | Total loss: [0.00012005] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48300  | Total loss: [0.00011911] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48400  | Total loss: [0.00011819] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48500  | Total loss: [0.00011726] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48600  | Total loss: [0.00011632] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48700  | Total loss: [0.00011541] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48800  | Total loss: [0.0001145] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 48900  | Total loss: [0.00011361] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49000  | Total loss: [0.00011273] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49100  | Total loss: [0.00011185] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49200  | Total loss: [0.00011097] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49300  | Total loss: [0.0001101] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49400  | Total loss: [0.00010925] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49500  | Total loss: [0.00010839] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49600  | Total loss: [0.00010754] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49700  | Total loss: [0.00010668] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49800  | Total loss: [0.00010585] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 49900  | Total loss: [0.00010502] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50000  | Total loss: [0.00010419] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50100  | Total loss: [0.00010337] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50200  | Total loss: [0.00010255] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50300  | Total loss: [0.00010175] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50400  | Total loss: [0.00010094] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50500  | Total loss: [0.00010016] | Learning rate: 9.999999974752427e-07\n",
      "Epoch: 50600  | Total loss: [9.9911595e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 50700  | Total loss: [9.983535e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 50800  | Total loss: [9.9741876e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 50900  | Total loss: [9.9643876e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51000  | Total loss: [9.954826e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51100  | Total loss: [9.946937e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51200  | Total loss: [9.9370525e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51300  | Total loss: [9.927436e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51400  | Total loss: [9.91779e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51500  | Total loss: [9.908669e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51600  | Total loss: [9.89932e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51700  | Total loss: [9.890083e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51800  | Total loss: [9.88043e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 51900  | Total loss: [9.870523e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52000  | Total loss: [9.8612116e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52100  | Total loss: [9.8525365e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52200  | Total loss: [9.842869e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52300  | Total loss: [9.833468e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52400  | Total loss: [9.823444e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52500  | Total loss: [9.813521e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52600  | Total loss: [9.804824e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52700  | Total loss: [9.794919e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52800  | Total loss: [9.784196e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 52900  | Total loss: [9.774848e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53000  | Total loss: [9.764933e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53100  | Total loss: [9.7550066e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53200  | Total loss: [9.746044e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53300  | Total loss: [9.735898e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53400  | Total loss: [9.7251526e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53500  | Total loss: [9.7167416e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53600  | Total loss: [9.7068725e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53700  | Total loss: [9.6967095e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53800  | Total loss: [9.687053e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 53900  | Total loss: [9.677045e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54000  | Total loss: [9.66716e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54100  | Total loss: [9.6567754e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54200  | Total loss: [9.6468575e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54300  | Total loss: [9.639263e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54400  | Total loss: [9.6284995e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54500  | Total loss: [9.618638e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54600  | Total loss: [9.609212e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54700  | Total loss: [9.5993055e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54800  | Total loss: [9.5893796e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 54900  | Total loss: [9.57919e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55000  | Total loss: [9.569878e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55100  | Total loss: [9.5602816e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55200  | Total loss: [9.551105e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55300  | Total loss: [9.541408e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55400  | Total loss: [9.5320305e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55500  | Total loss: [9.521765e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55600  | Total loss: [9.510798e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55700  | Total loss: [9.501903e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55800  | Total loss: [9.493472e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 55900  | Total loss: [9.483567e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56000  | Total loss: [9.472561e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56100  | Total loss: [9.462517e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56200  | Total loss: [9.4537165e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56300  | Total loss: [9.443288e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56400  | Total loss: [9.433489e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56500  | Total loss: [9.4260766e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56600  | Total loss: [9.415053e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56700  | Total loss: [9.405209e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56800  | Total loss: [9.397068e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 56900  | Total loss: [9.3873496e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57000  | Total loss: [9.376342e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57100  | Total loss: [9.365872e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57200  | Total loss: [9.3590235e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57300  | Total loss: [9.349078e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57400  | Total loss: [9.3395225e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57500  | Total loss: [9.330164e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57600  | Total loss: [9.320143e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57700  | Total loss: [9.3095805e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57800  | Total loss: [9.299905e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 57900  | Total loss: [9.291754e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58000  | Total loss: [9.2818096e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58100  | Total loss: [9.271953e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58200  | Total loss: [9.263178e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58300  | Total loss: [9.2533955e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58400  | Total loss: [9.244057e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58500  | Total loss: [9.233868e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58600  | Total loss: [9.225304e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58700  | Total loss: [9.216505e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58800  | Total loss: [9.207261e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 58900  | Total loss: [9.197182e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59000  | Total loss: [9.188085e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59100  | Total loss: [9.1789465e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59200  | Total loss: [9.168761e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59300  | Total loss: [9.159303e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59400  | Total loss: [9.150463e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59500  | Total loss: [9.142192e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59600  | Total loss: [9.132424e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59700  | Total loss: [9.122339e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59800  | Total loss: [9.112777e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 59900  | Total loss: [9.1032955e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60000  | Total loss: [9.0945294e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60100  | Total loss: [9.084309e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60200  | Total loss: [9.0765774e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60300  | Total loss: [9.067459e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60400  | Total loss: [9.056513e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60500  | Total loss: [9.04796e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60600  | Total loss: [9.039146e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60700  | Total loss: [9.029965e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60800  | Total loss: [9.021549e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 60900  | Total loss: [9.010638e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61000  | Total loss: [9.0011235e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61100  | Total loss: [8.9933725e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61200  | Total loss: [8.98436e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61300  | Total loss: [8.973848e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61400  | Total loss: [8.9653666e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61500  | Total loss: [8.9549976e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61600  | Total loss: [8.9464535e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61700  | Total loss: [8.937752e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61800  | Total loss: [8.928111e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 61900  | Total loss: [8.919677e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62000  | Total loss: [8.9101035e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62100  | Total loss: [8.900639e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62200  | Total loss: [8.8928704e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62300  | Total loss: [8.883048e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62400  | Total loss: [8.8723944e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62500  | Total loss: [8.8641864e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62600  | Total loss: [8.854583e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62700  | Total loss: [8.846827e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62800  | Total loss: [8.8376735e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 62900  | Total loss: [8.827702e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63000  | Total loss: [8.820111e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63100  | Total loss: [8.810256e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63200  | Total loss: [8.800672e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63300  | Total loss: [8.7927125e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63400  | Total loss: [8.7826105e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63500  | Total loss: [8.7749926e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63600  | Total loss: [8.7658875e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63700  | Total loss: [8.7566616e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63800  | Total loss: [8.747128e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 63900  | Total loss: [8.739e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64000  | Total loss: [8.7296874e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64100  | Total loss: [8.719642e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64200  | Total loss: [8.7124055e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64300  | Total loss: [8.702774e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64400  | Total loss: [8.693738e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64500  | Total loss: [8.6851505e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64600  | Total loss: [8.6768276e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64700  | Total loss: [8.6671214e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64800  | Total loss: [8.658949e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 64900  | Total loss: [8.6492815e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65000  | Total loss: [8.639592e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65100  | Total loss: [8.632759e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65200  | Total loss: [8.622788e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65300  | Total loss: [8.614035e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65400  | Total loss: [8.604966e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65500  | Total loss: [8.595948e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65600  | Total loss: [8.587391e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65700  | Total loss: [8.579639e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65800  | Total loss: [8.569987e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 65900  | Total loss: [8.5611166e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66000  | Total loss: [8.5526764e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66100  | Total loss: [8.544277e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66200  | Total loss: [8.5353706e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66300  | Total loss: [8.526355e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66400  | Total loss: [8.5185e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66500  | Total loss: [8.508392e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66600  | Total loss: [8.4998326e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66700  | Total loss: [8.491058e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66800  | Total loss: [8.4816704e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 66900  | Total loss: [8.473666e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67000  | Total loss: [8.464559e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67100  | Total loss: [8.456468e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67200  | Total loss: [8.4479325e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67300  | Total loss: [8.438037e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67400  | Total loss: [8.430358e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67500  | Total loss: [8.4212865e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67600  | Total loss: [8.4124986e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67700  | Total loss: [8.403971e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67800  | Total loss: [8.395775e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 67900  | Total loss: [8.386399e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68000  | Total loss: [8.377153e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68100  | Total loss: [8.369881e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68200  | Total loss: [8.361161e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68300  | Total loss: [8.353513e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68400  | Total loss: [8.3449995e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68500  | Total loss: [8.336619e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68600  | Total loss: [8.326519e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68700  | Total loss: [8.318498e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68800  | Total loss: [8.309318e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 68900  | Total loss: [8.301125e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69000  | Total loss: [8.292355e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69100  | Total loss: [8.283738e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69200  | Total loss: [8.275383e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69300  | Total loss: [8.267255e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69400  | Total loss: [8.2581464e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69500  | Total loss: [8.2500126e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69600  | Total loss: [8.241435e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69700  | Total loss: [8.232354e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69800  | Total loss: [8.225191e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 69900  | Total loss: [8.21601e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70000  | Total loss: [8.207685e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70100  | Total loss: [8.1999264e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70200  | Total loss: [8.189995e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70300  | Total loss: [8.181301e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70400  | Total loss: [8.174766e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70500  | Total loss: [8.166614e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70600  | Total loss: [8.157942e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70700  | Total loss: [8.148847e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70800  | Total loss: [8.140943e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 70900  | Total loss: [8.1312006e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71000  | Total loss: [8.1243765e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71100  | Total loss: [8.1164144e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71200  | Total loss: [8.106964e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71300  | Total loss: [8.100104e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71400  | Total loss: [8.090209e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71500  | Total loss: [8.081164e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71600  | Total loss: [8.073864e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71700  | Total loss: [8.065668e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71800  | Total loss: [8.056529e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 71900  | Total loss: [8.048848e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72000  | Total loss: [8.040242e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72100  | Total loss: [8.032151e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72200  | Total loss: [8.023712e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72300  | Total loss: [8.016159e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72400  | Total loss: [8.0077865e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72500  | Total loss: [7.999364e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72600  | Total loss: [7.9902136e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72700  | Total loss: [7.9825724e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72800  | Total loss: [7.9749756e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 72900  | Total loss: [7.965857e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73000  | Total loss: [7.958165e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73100  | Total loss: [7.949931e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73200  | Total loss: [7.941321e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73300  | Total loss: [7.9336e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73400  | Total loss: [7.92468e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73500  | Total loss: [7.917237e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73600  | Total loss: [7.908425e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73700  | Total loss: [7.90097e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73800  | Total loss: [7.8927056e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 73900  | Total loss: [7.8842015e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74000  | Total loss: [7.8760626e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74100  | Total loss: [7.866818e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74200  | Total loss: [7.8599856e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74300  | Total loss: [7.853341e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74400  | Total loss: [7.8437915e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74500  | Total loss: [7.834883e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74600  | Total loss: [7.827533e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74700  | Total loss: [7.8193436e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74800  | Total loss: [7.810768e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 74900  | Total loss: [7.803261e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75000  | Total loss: [7.795081e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75100  | Total loss: [7.788177e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75200  | Total loss: [7.779842e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75300  | Total loss: [7.771106e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75400  | Total loss: [7.762972e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75500  | Total loss: [7.756408e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75600  | Total loss: [7.746376e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75700  | Total loss: [7.739087e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75800  | Total loss: [7.730971e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 75900  | Total loss: [7.722786e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76000  | Total loss: [7.716296e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76100  | Total loss: [7.708549e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76200  | Total loss: [7.69942e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76300  | Total loss: [7.6911834e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76400  | Total loss: [7.683554e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76500  | Total loss: [7.675696e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76600  | Total loss: [7.667872e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76700  | Total loss: [7.659785e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76800  | Total loss: [7.651096e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 76900  | Total loss: [7.64395e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77000  | Total loss: [7.636319e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77100  | Total loss: [7.627662e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77200  | Total loss: [7.621154e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77300  | Total loss: [7.611059e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77400  | Total loss: [7.60409e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77500  | Total loss: [7.596416e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77600  | Total loss: [7.588559e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77700  | Total loss: [7.5811375e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77800  | Total loss: [7.5740194e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 77900  | Total loss: [7.5658514e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78000  | Total loss: [7.5581236e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78100  | Total loss: [7.550494e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78200  | Total loss: [7.5417e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78300  | Total loss: [7.533968e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78400  | Total loss: [7.527877e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78500  | Total loss: [7.518613e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78600  | Total loss: [7.5101554e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78700  | Total loss: [7.50312e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78800  | Total loss: [7.495912e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 78900  | Total loss: [7.48906e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79000  | Total loss: [7.480106e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79100  | Total loss: [7.472622e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79200  | Total loss: [7.4643685e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79300  | Total loss: [7.45668e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79400  | Total loss: [7.448931e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79500  | Total loss: [7.441087e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79600  | Total loss: [7.43383e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79700  | Total loss: [7.4252595e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79800  | Total loss: [7.4182346e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 79900  | Total loss: [7.410904e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80000  | Total loss: [7.4027754e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80100  | Total loss: [7.394253e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80200  | Total loss: [7.386823e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80300  | Total loss: [7.380598e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80400  | Total loss: [7.372741e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80500  | Total loss: [7.3648975e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80600  | Total loss: [7.356824e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80700  | Total loss: [7.350677e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80800  | Total loss: [7.3422045e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 80900  | Total loss: [7.3337294e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81000  | Total loss: [7.3274656e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81100  | Total loss: [7.3191935e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81200  | Total loss: [7.310404e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81300  | Total loss: [7.3035495e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81400  | Total loss: [7.296415e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81500  | Total loss: [7.289512e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81600  | Total loss: [7.282064e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81700  | Total loss: [7.2737785e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81800  | Total loss: [7.265908e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 81900  | Total loss: [7.259431e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82000  | Total loss: [7.251166e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82100  | Total loss: [7.24349e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82200  | Total loss: [7.236057e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82300  | Total loss: [7.228526e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82400  | Total loss: [7.219965e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82500  | Total loss: [7.2139774e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82600  | Total loss: [7.206438e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82700  | Total loss: [7.199547e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82800  | Total loss: [7.1927505e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 82900  | Total loss: [7.18456e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83000  | Total loss: [7.175555e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83100  | Total loss: [7.168353e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83200  | Total loss: [7.1617316e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83300  | Total loss: [7.1539536e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83400  | Total loss: [7.146548e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83500  | Total loss: [7.139047e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83600  | Total loss: [7.131776e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83700  | Total loss: [7.125677e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83800  | Total loss: [7.117258e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 83900  | Total loss: [7.110028e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84000  | Total loss: [7.103204e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84100  | Total loss: [7.095588e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84200  | Total loss: [7.088427e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84300  | Total loss: [7.07996e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84400  | Total loss: [7.07384e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84500  | Total loss: [7.064915e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84600  | Total loss: [7.058638e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84700  | Total loss: [7.051278e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84800  | Total loss: [7.0436894e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 84900  | Total loss: [7.036657e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85000  | Total loss: [7.029038e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85100  | Total loss: [7.022132e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85200  | Total loss: [7.014898e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85300  | Total loss: [7.008815e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85400  | Total loss: [7.001161e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85500  | Total loss: [6.9932226e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85600  | Total loss: [6.985873e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85700  | Total loss: [6.977674e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85800  | Total loss: [6.970801e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 85900  | Total loss: [6.964664e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86000  | Total loss: [6.9568254e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86100  | Total loss: [6.948881e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86200  | Total loss: [6.9418e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86300  | Total loss: [6.936615e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86400  | Total loss: [6.927882e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86500  | Total loss: [6.921314e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86600  | Total loss: [6.9143425e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86700  | Total loss: [6.906298e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86800  | Total loss: [6.899847e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 86900  | Total loss: [6.892924e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87000  | Total loss: [6.885328e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87100  | Total loss: [6.878859e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87200  | Total loss: [6.870748e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87300  | Total loss: [6.863588e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87400  | Total loss: [6.8574984e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87500  | Total loss: [6.850226e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87600  | Total loss: [6.842273e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87700  | Total loss: [6.835875e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87800  | Total loss: [6.8289446e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 87900  | Total loss: [6.820683e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88000  | Total loss: [6.814769e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88100  | Total loss: [6.806945e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88200  | Total loss: [6.7998255e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88300  | Total loss: [6.7924586e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88400  | Total loss: [6.787173e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88500  | Total loss: [6.779215e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88600  | Total loss: [6.771216e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88700  | Total loss: [6.765585e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88800  | Total loss: [6.758568e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 88900  | Total loss: [6.750649e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89000  | Total loss: [6.74514e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89100  | Total loss: [6.7373236e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89200  | Total loss: [6.7305016e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89300  | Total loss: [6.7244866e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89400  | Total loss: [6.716573e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89500  | Total loss: [6.708695e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89600  | Total loss: [6.7017536e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89700  | Total loss: [6.696505e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89800  | Total loss: [6.687275e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 89900  | Total loss: [6.680686e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90000  | Total loss: [6.674632e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90100  | Total loss: [6.6671775e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90200  | Total loss: [6.660079e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90300  | Total loss: [6.6544104e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90400  | Total loss: [6.647017e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90500  | Total loss: [6.639893e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90600  | Total loss: [6.63428e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90700  | Total loss: [6.626179e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90800  | Total loss: [6.619432e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 90900  | Total loss: [6.6130466e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91000  | Total loss: [6.60626e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91100  | Total loss: [6.598239e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91200  | Total loss: [6.591408e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91300  | Total loss: [6.58649e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91400  | Total loss: [6.5793785e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91500  | Total loss: [6.5718385e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91600  | Total loss: [6.565558e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91700  | Total loss: [6.558119e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91800  | Total loss: [6.550899e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 91900  | Total loss: [6.544766e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92000  | Total loss: [6.537461e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92100  | Total loss: [6.5309876e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92200  | Total loss: [6.524402e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92300  | Total loss: [6.5175765e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92400  | Total loss: [6.5106025e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92500  | Total loss: [6.5041066e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92600  | Total loss: [6.497838e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92700  | Total loss: [6.490878e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92800  | Total loss: [6.483857e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 92900  | Total loss: [6.477671e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93000  | Total loss: [6.470783e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93100  | Total loss: [6.464831e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93200  | Total loss: [6.456547e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93300  | Total loss: [6.450202e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93400  | Total loss: [6.4444575e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93500  | Total loss: [6.437595e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93600  | Total loss: [6.430892e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93700  | Total loss: [6.424291e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93800  | Total loss: [6.416829e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 93900  | Total loss: [6.4106614e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94000  | Total loss: [6.403987e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94100  | Total loss: [6.397924e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94200  | Total loss: [6.3907566e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94300  | Total loss: [6.384692e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94400  | Total loss: [6.37697e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94500  | Total loss: [6.3717365e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94600  | Total loss: [6.364976e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94700  | Total loss: [6.357192e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94800  | Total loss: [6.350777e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 94900  | Total loss: [6.3442785e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95000  | Total loss: [6.337613e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95100  | Total loss: [6.331894e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95200  | Total loss: [6.3251064e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95300  | Total loss: [6.317934e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95400  | Total loss: [6.312004e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95500  | Total loss: [6.304905e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95600  | Total loss: [6.2990635e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95700  | Total loss: [6.2905514e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95800  | Total loss: [6.285117e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 95900  | Total loss: [6.2782776e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96000  | Total loss: [6.2720836e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96100  | Total loss: [6.2661245e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96200  | Total loss: [6.259789e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96300  | Total loss: [6.252119e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96400  | Total loss: [6.246633e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96500  | Total loss: [6.240031e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96600  | Total loss: [6.233067e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96700  | Total loss: [6.2268715e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96800  | Total loss: [6.2203355e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 96900  | Total loss: [6.2142506e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97000  | Total loss: [6.2081905e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97100  | Total loss: [6.2022846e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97200  | Total loss: [6.195179e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97300  | Total loss: [6.189175e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97400  | Total loss: [6.1812476e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97500  | Total loss: [6.176246e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97600  | Total loss: [6.1700404e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97700  | Total loss: [6.163302e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97800  | Total loss: [6.1569546e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 97900  | Total loss: [6.149774e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98000  | Total loss: [6.1444916e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98100  | Total loss: [6.1370185e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98200  | Total loss: [6.1303566e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98300  | Total loss: [6.1245264e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98400  | Total loss: [6.118672e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98500  | Total loss: [6.1119485e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98600  | Total loss: [6.1052335e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98700  | Total loss: [6.099707e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98800  | Total loss: [6.0935967e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 98900  | Total loss: [6.087217e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99000  | Total loss: [6.0806953e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99100  | Total loss: [6.0736664e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99200  | Total loss: [6.068421e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99300  | Total loss: [6.061743e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99400  | Total loss: [6.0553564e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99500  | Total loss: [6.0489354e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99600  | Total loss: [6.043397e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99700  | Total loss: [6.037042e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99800  | Total loss: [6.0298637e-05] | Learning rate: 1.0000000116860974e-07\n",
      "Epoch: 99900  | Total loss: [6.023874e-05] | Learning rate: 1.0000000116860974e-07\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "epochs = 100000\n",
    "epoch_count = [0]\n",
    "loss_track = [1]\n",
    "pde_loss_track = [0]\n",
    "boundary_loss_track = [0]\n",
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=my_first_model.parameters(),lr= learning_rate, weight_decay= 0.01)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Putting the model in training mode\n",
    "    my_first_model.train()\n",
    "\n",
    "    # # Setting the gradients to zero\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    y_preds = my_first_model(x_input)\n",
    "    # Calculating the derivative d2y_dx2\n",
    "    dy_dx = torch.autograd.grad(outputs=y_preds, inputs= x_input, grad_outputs= torch.ones_like(y_preds), create_graph= True)[0]\n",
    "    # and finally\n",
    "    d2y_dx2 = torch.autograd.grad(outputs= dy_dx, inputs= x_input, grad_outputs=torch.ones_like(dy_dx), create_graph=True)[0]\n",
    "    # Caluclating the y at the boundary value\n",
    "\n",
    "    x_bc = torch.tensor(0.0, requires_grad = True).unsqueeze(dim=-1)\n",
    "    y_bc = torch.tensor(1.0, requires_grad = False)\n",
    "    dy_dx_bc = torch.tensor(1.0, requires_grad=False)\n",
    "\n",
    "    y_pred_bc = my_first_model(x_bc)\n",
    "    dy_dx_bc_pred = torch.autograd.grad(outputs = y_pred_bc, inputs = x_bc, grad_outputs= torch.ones_like(y_pred_bc), create_graph = True)[0]\n",
    "\n",
    "    # SO the loss is to be define here then\n",
    "    loss_diff_eq = torch.mean((d2y_dx2+y_preds)**2) \n",
    "    loss_bc_1 = (y_pred_bc-y_bc)**2\n",
    "    loss_bc_2 = (dy_dx_bc_pred - dy_dx_bc)**2\n",
    "    boundary_loss = loss_bc_1 + loss_bc_2\n",
    "\n",
    "\n",
    "    total_loss = 10*loss_diff_eq + 1*(boundary_loss)\n",
    "\n",
    "    # I am defining learning rate in a special way. The lr will be two order lower than the total_loss\n",
    "\n",
    "    \n",
    "    learning_rate = lower_learning_rate(order(total_loss.detach().numpy()[0]))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    \n",
    "    \n",
    "    # Setting the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculating the backward pass\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Updating the model\n",
    "    optimizer.step()\n",
    "    \n",
    "    # epoch_count.append(i)\n",
    "    # loss_track.append(total_loss.detach().numpy())\n",
    "    # pde_loss_track.append(loss_diff_eq.detach().numpy())\n",
    "    # boundary_loss_track.append(boundary_loss.detach().numpy())\n",
    "    # print(f\"Epoch: {i}  | Total loss: {total_loss.detach().numpy()} | Learning rate: {learning_rate}\")\n",
    "\n",
    "    if i%100==0:\n",
    "        epoch_count.append(i)\n",
    "        loss_track.append(total_loss.detach().numpy())\n",
    "        pde_loss_track.append(loss_diff_eq.detach().numpy())\n",
    "        boundary_loss_track.append(boundary_loss.detach().numpy())\n",
    "        print(f\"Epoch: {i}  | Total loss: {total_loss.detach().numpy()} | Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ef715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.0167],\n",
      "        [0.0334],\n",
      "        [0.0502],\n",
      "        [0.0669],\n",
      "        [0.0836],\n",
      "        [0.1003],\n",
      "        [0.1171],\n",
      "        [0.1338],\n",
      "        [0.1505],\n",
      "        [0.1672],\n",
      "        [0.1839],\n",
      "        [0.2007],\n",
      "        [0.2174],\n",
      "        [0.2341],\n",
      "        [0.2508],\n",
      "        [0.2676],\n",
      "        [0.2843],\n",
      "        [0.3010],\n",
      "        [0.3177],\n",
      "        [0.3344],\n",
      "        [0.3512],\n",
      "        [0.3679],\n",
      "        [0.3846],\n",
      "        [0.4013],\n",
      "        [0.4181],\n",
      "        [0.4348],\n",
      "        [0.4515],\n",
      "        [0.4682],\n",
      "        [0.4849],\n",
      "        [0.5017],\n",
      "        [0.5184],\n",
      "        [0.5351],\n",
      "        [0.5518],\n",
      "        [0.5686],\n",
      "        [0.5853],\n",
      "        [0.6020],\n",
      "        [0.6187],\n",
      "        [0.6355],\n",
      "        [0.6522],\n",
      "        [0.6689],\n",
      "        [0.6856],\n",
      "        [0.7023],\n",
      "        [0.7191],\n",
      "        [0.7358],\n",
      "        [0.7525],\n",
      "        [0.7692],\n",
      "        [0.7860],\n",
      "        [0.8027],\n",
      "        [0.8194],\n",
      "        [0.8361],\n",
      "        [0.8528],\n",
      "        [0.8696],\n",
      "        [0.8863],\n",
      "        [0.9030],\n",
      "        [0.9197],\n",
      "        [0.9365],\n",
      "        [0.9532],\n",
      "        [0.9699],\n",
      "        [0.9866],\n",
      "        [1.0033],\n",
      "        [1.0201],\n",
      "        [1.0368],\n",
      "        [1.0535],\n",
      "        [1.0702],\n",
      "        [1.0870],\n",
      "        [1.1037],\n",
      "        [1.1204],\n",
      "        [1.1371],\n",
      "        [1.1538],\n",
      "        [1.1706],\n",
      "        [1.1873],\n",
      "        [1.2040],\n",
      "        [1.2207],\n",
      "        [1.2375],\n",
      "        [1.2542],\n",
      "        [1.2709],\n",
      "        [1.2876],\n",
      "        [1.3043],\n",
      "        [1.3211],\n",
      "        [1.3378],\n",
      "        [1.3545],\n",
      "        [1.3712],\n",
      "        [1.3880],\n",
      "        [1.4047],\n",
      "        [1.4214],\n",
      "        [1.4381],\n",
      "        [1.4548],\n",
      "        [1.4716],\n",
      "        [1.4883],\n",
      "        [1.5050],\n",
      "        [1.5217],\n",
      "        [1.5385],\n",
      "        [1.5552],\n",
      "        [1.5719],\n",
      "        [1.5886],\n",
      "        [1.6054],\n",
      "        [1.6221],\n",
      "        [1.6388],\n",
      "        [1.6555],\n",
      "        [1.6722],\n",
      "        [1.6890],\n",
      "        [1.7057],\n",
      "        [1.7224],\n",
      "        [1.7391],\n",
      "        [1.7559],\n",
      "        [1.7726],\n",
      "        [1.7893],\n",
      "        [1.8060],\n",
      "        [1.8227],\n",
      "        [1.8395],\n",
      "        [1.8562],\n",
      "        [1.8729],\n",
      "        [1.8896],\n",
      "        [1.9064],\n",
      "        [1.9231],\n",
      "        [1.9398],\n",
      "        [1.9565],\n",
      "        [1.9732],\n",
      "        [1.9900],\n",
      "        [2.0067],\n",
      "        [2.0234],\n",
      "        [2.0401],\n",
      "        [2.0569],\n",
      "        [2.0736],\n",
      "        [2.0903],\n",
      "        [2.1070],\n",
      "        [2.1237],\n",
      "        [2.1405],\n",
      "        [2.1572],\n",
      "        [2.1739],\n",
      "        [2.1906],\n",
      "        [2.2074],\n",
      "        [2.2241],\n",
      "        [2.2408],\n",
      "        [2.2575],\n",
      "        [2.2742],\n",
      "        [2.2910],\n",
      "        [2.3077],\n",
      "        [2.3244],\n",
      "        [2.3411],\n",
      "        [2.3579],\n",
      "        [2.3746],\n",
      "        [2.3913],\n",
      "        [2.4080],\n",
      "        [2.4247],\n",
      "        [2.4415],\n",
      "        [2.4582],\n",
      "        [2.4749],\n",
      "        [2.4916],\n",
      "        [2.5084],\n",
      "        [2.5251],\n",
      "        [2.5418],\n",
      "        [2.5585],\n",
      "        [2.5753],\n",
      "        [2.5920],\n",
      "        [2.6087],\n",
      "        [2.6254],\n",
      "        [2.6421],\n",
      "        [2.6589],\n",
      "        [2.6756],\n",
      "        [2.6923],\n",
      "        [2.7090],\n",
      "        [2.7258],\n",
      "        [2.7425],\n",
      "        [2.7592],\n",
      "        [2.7759],\n",
      "        [2.7926],\n",
      "        [2.8094],\n",
      "        [2.8261],\n",
      "        [2.8428],\n",
      "        [2.8595],\n",
      "        [2.8763],\n",
      "        [2.8930],\n",
      "        [2.9097],\n",
      "        [2.9264],\n",
      "        [2.9431],\n",
      "        [2.9599],\n",
      "        [2.9766],\n",
      "        [2.9933],\n",
      "        [3.0100],\n",
      "        [3.0268],\n",
      "        [3.0435],\n",
      "        [3.0602],\n",
      "        [3.0769],\n",
      "        [3.0936],\n",
      "        [3.1104],\n",
      "        [3.1271],\n",
      "        [3.1438],\n",
      "        [3.1605],\n",
      "        [3.1773],\n",
      "        [3.1940],\n",
      "        [3.2107],\n",
      "        [3.2274],\n",
      "        [3.2441],\n",
      "        [3.2609],\n",
      "        [3.2776],\n",
      "        [3.2943],\n",
      "        [3.3110],\n",
      "        [3.3278],\n",
      "        [3.3445],\n",
      "        [3.3612],\n",
      "        [3.3779],\n",
      "        [3.3946],\n",
      "        [3.4114],\n",
      "        [3.4281],\n",
      "        [3.4448],\n",
      "        [3.4615],\n",
      "        [3.4783],\n",
      "        [3.4950],\n",
      "        [3.5117],\n",
      "        [3.5284],\n",
      "        [3.5452],\n",
      "        [3.5619],\n",
      "        [3.5786],\n",
      "        [3.5953],\n",
      "        [3.6120],\n",
      "        [3.6288],\n",
      "        [3.6455],\n",
      "        [3.6622],\n",
      "        [3.6789],\n",
      "        [3.6957],\n",
      "        [3.7124],\n",
      "        [3.7291],\n",
      "        [3.7458],\n",
      "        [3.7625],\n",
      "        [3.7793],\n",
      "        [3.7960],\n",
      "        [3.8127],\n",
      "        [3.8294],\n",
      "        [3.8462],\n",
      "        [3.8629],\n",
      "        [3.8796],\n",
      "        [3.8963],\n",
      "        [3.9130],\n",
      "        [3.9298],\n",
      "        [3.9465],\n",
      "        [3.9632],\n",
      "        [3.9799],\n",
      "        [3.9967],\n",
      "        [4.0134],\n",
      "        [4.0301],\n",
      "        [4.0468],\n",
      "        [4.0635],\n",
      "        [4.0803],\n",
      "        [4.0970],\n",
      "        [4.1137],\n",
      "        [4.1304],\n",
      "        [4.1472],\n",
      "        [4.1639],\n",
      "        [4.1806],\n",
      "        [4.1973],\n",
      "        [4.2140],\n",
      "        [4.2308],\n",
      "        [4.2475],\n",
      "        [4.2642],\n",
      "        [4.2809],\n",
      "        [4.2977],\n",
      "        [4.3144],\n",
      "        [4.3311],\n",
      "        [4.3478],\n",
      "        [4.3645],\n",
      "        [4.3813],\n",
      "        [4.3980],\n",
      "        [4.4147],\n",
      "        [4.4314],\n",
      "        [4.4482],\n",
      "        [4.4649],\n",
      "        [4.4816],\n",
      "        [4.4983],\n",
      "        [4.5150],\n",
      "        [4.5318],\n",
      "        [4.5485],\n",
      "        [4.5652],\n",
      "        [4.5819],\n",
      "        [4.5987],\n",
      "        [4.6154],\n",
      "        [4.6321],\n",
      "        [4.6488],\n",
      "        [4.6656],\n",
      "        [4.6823],\n",
      "        [4.6990],\n",
      "        [4.7157],\n",
      "        [4.7324],\n",
      "        [4.7492],\n",
      "        [4.7659],\n",
      "        [4.7826],\n",
      "        [4.7993],\n",
      "        [4.8161],\n",
      "        [4.8328],\n",
      "        [4.8495],\n",
      "        [4.8662],\n",
      "        [4.8829],\n",
      "        [4.8997],\n",
      "        [4.9164],\n",
      "        [4.9331],\n",
      "        [4.9498],\n",
      "        [4.9666],\n",
      "        [4.9833],\n",
      "        [5.0000]])\n"
     ]
    }
   ],
   "source": [
    "validation_points = torch.tensor(np.linspace(0,5,300), requires_grad=False,dtype=torch.float32)\n",
    "validation_points = validation_points.unsqueeze(dim=1)\n",
    "print(validation_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31dc9beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ab5623e1730>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = my_first_model(validation_points)\n",
    "plt.plot(validation_points.detach().numpy(),output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87feec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(pde_loss_track,epoch_count)\n",
    "plt.plot(epoch_count[-700:],loss_track[-700:])\n",
    "# plt.savefig('1_2.png', dpi=300, bbox_inches='tight')\n",
    "# print(pde_loss_track)\n",
    "print(len(epoch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a41d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
