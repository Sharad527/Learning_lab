{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf37540b",
   "metadata": {},
   "source": [
    "# We will solve the Simple Harmonic equation here\n",
    "$$\n",
    "\\frac{d^2 }{dx^2}y(x)+5y(x)=0\n",
    "$$\n",
    "for the boundary conditions $y(0)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9f26662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  2.2222,  4.4444,  6.6667,  8.8889, 11.1111, 13.3333, 15.5556,\n",
      "        17.7778, 20.0000], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[ 0.0000],\n",
      "        [ 2.2222],\n",
      "        [ 4.4444],\n",
      "        [ 6.6667],\n",
      "        [ 8.8889],\n",
      "        [11.1111],\n",
      "        [13.3333],\n",
      "        [15.5556],\n",
      "        [17.7778],\n",
      "        [20.0000]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LEts start by defining the domain in which I want the solution of this differntial equation\n",
    "x = torch.tensor(np.linspace(0,20,10),requires_grad=True)\n",
    "x = x.float()\n",
    "print(x)\n",
    "x_input = x.unsqueeze(dim=1)\n",
    "\n",
    "x_input.requires_grad_(True)\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de35b4",
   "metadata": {},
   "source": [
    "## Now I will define neural network and everything else will follow accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d0fbdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, in_layer = int, out_layer = int, num_of_layer = int, hidden_size = int):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_layer, hidden_size)\n",
    "        self.activation = nn.Tanh() # For smooth derivatives\n",
    "\n",
    "        # Creating scalable model\n",
    "        hidden_layer = []\n",
    "        for _ in range(num_of_layer-1):\n",
    "            hidden_layer.append(nn.Linear(in_features = hidden_size, out_features = hidden_size))\n",
    "            hidden_layer.append(nn.Tanh())\n",
    "        \n",
    "        # Registering the list of layers as module list\n",
    "        self.hidden_stack = nn.ModuleList(hidden_layer)\n",
    "\n",
    "\n",
    "        #Defining the output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, out_layer)\n",
    "    \n",
    "\n",
    "    #Forward pass\n",
    "    def forward(self, x: torch.tensor):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "\n",
    "        for layer in self.hidden_stack:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ab4989ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if the code is working\n",
    "torch.manual_seed(10)\n",
    "\n",
    "# Creating an instance of our neural network\n",
    "my_first_model = PINN(1,1,5,20)\n",
    "\n",
    "\n",
    "\n",
    "y_preds = my_first_model(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e822aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0318],\n",
      "        [0.0320],\n",
      "        [0.0455],\n",
      "        [0.0540],\n",
      "        [0.0585],\n",
      "        [0.0612],\n",
      "        [0.0630],\n",
      "        [0.0641],\n",
      "        [0.0646],\n",
      "        [0.0647]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f749f1",
   "metadata": {},
   "source": [
    "# Moving towards loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068cbd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.4031e-03],\n",
      "        [-1.9148e-03],\n",
      "        [ 1.0744e-03],\n",
      "        [ 7.2771e-04],\n",
      "        [ 4.5516e-04],\n",
      "        [ 2.9316e-04],\n",
      "        [ 1.8107e-04],\n",
      "        [ 1.0700e-04],\n",
      "        [ 6.1373e-05],\n",
      "        [ 3.4559e-05]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dy_dx = torch.autograd.grad(outputs=y_preds, inputs= x_input, grad_outputs= torch.ones_like(y_preds), create_graph= True)[0]\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "627c490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.7948e-03],\n",
      "        [ 3.5722e-03],\n",
      "        [ 1.9459e-05],\n",
      "        [-1.6751e-04],\n",
      "        [-8.9226e-05],\n",
      "        [-6.0244e-05],\n",
      "        [-4.1249e-05],\n",
      "        [-2.6159e-05],\n",
      "        [-1.5647e-05],\n",
      "        [-9.0348e-06]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d2y_dx2 = torch.autograd.grad(outputs= dy_dx, inputs= x_input, grad_outputs=torch.ones_like(dy_dx), create_graph=True)[0]\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4db168",
   "metadata": {},
   "source": [
    "Now at this point I am only left with one last ingredient to create the loss function. The boundary conditions. Since, boundary condition of the form\n",
    "$$\n",
    "\\mathcal{L}_{BC} = ||y_{pred}|_{x=0}-y_{constant}|_{x=0}||^2\n",
    "$$\n",
    "This means I need another tensor which is the point x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d62afdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the boundary point\n",
    "x_bc = torch.tensor(0.0, requires_grad = False).unsqueeze(dim=-1)\n",
    "\n",
    "# Similarly I need to setup the y point which has a constant value\n",
    "y_bc = torch.tensor(1.0, requires_grad = False)\n",
    "\n",
    "# I understand that I need to output the value of network at the pont x_bc which should always be equal to y_bc\n",
    "\n",
    "y_pred_bc = my_first_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ad740ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0206], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pred_bc = my_first_model(x_bc)\n",
    "print(y_pred_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5381a",
   "metadata": {},
   "source": [
    "# Let's create loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661068b",
   "metadata": {},
   "source": [
    "The first loss if of the differential equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f83130df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_diff_eq = torch.mean((d2y_dx2+5*y_preds)**2)\n",
    "print(loss_diff_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed3f15",
   "metadata": {},
   "source": [
    "Now I have to define the loss of the boundary condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f17280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_bc = (y_pred_bc-y_bc)\n",
    "# print(loss_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81072e5",
   "metadata": {},
   "source": [
    "Hence the total loss is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e59e5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9390], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "total_loss = loss_diff_eq + loss_bc\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3ced76",
   "metadata": {},
   "source": [
    "I have to choose an optimiser as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=my_first_model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71e26c",
   "metadata": {},
   "source": [
    "# Time to train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "96de69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  | Total loss: [1.0144316]\n",
      "Epoch: 100  | Total loss: [0.00200118]\n",
      "Epoch: 200  | Total loss: [3.94711e-05]\n",
      "Epoch: 300  | Total loss: [2.2506374e-05]\n",
      "Epoch: 400  | Total loss: [1.2553251e-05]\n",
      "Epoch: 500  | Total loss: [6.844591e-06]\n",
      "Epoch: 600  | Total loss: [3.6991094e-06]\n",
      "Epoch: 700  | Total loss: [2.0678347e-06]\n",
      "Epoch: 800  | Total loss: [1.2777198e-06]\n",
      "Epoch: 900  | Total loss: [9.141613e-07]\n",
      "Epoch: 1000  | Total loss: [0.00258687]\n",
      "Epoch: 1100  | Total loss: [4.9120426e-06]\n",
      "Epoch: 1200  | Total loss: [1.7052471e-06]\n",
      "Epoch: 1300  | Total loss: [3.6655358e-06]\n",
      "Epoch: 1400  | Total loss: [5.8145833e-06]\n",
      "Epoch: 1500  | Total loss: [5.053242e-07]\n",
      "Epoch: 1600  | Total loss: [9.3623385e-05]\n",
      "Epoch: 1700  | Total loss: [5.8890885e-07]\n",
      "Epoch: 1800  | Total loss: [3.3192198e-06]\n",
      "Epoch: 1900  | Total loss: [9.130246e-06]\n",
      "Epoch: 2000  | Total loss: [1.0731127e-07]\n",
      "Epoch: 2100  | Total loss: [0.00073344]\n",
      "Epoch: 2200  | Total loss: [1.5176474e-07]\n",
      "Epoch: 2300  | Total loss: [4.551071e-08]\n",
      "Epoch: 2400  | Total loss: [0.00185866]\n",
      "Epoch: 2500  | Total loss: [8.1090747e-07]\n",
      "Epoch: 2600  | Total loss: [2.6326104e-08]\n",
      "Epoch: 2700  | Total loss: [0.01020797]\n",
      "Epoch: 2800  | Total loss: [5.1358927e-07]\n",
      "Epoch: 2900  | Total loss: [3.1219102e-08]\n",
      "Epoch: 3000  | Total loss: [2.616016e-08]\n",
      "Epoch: 3100  | Total loss: [5.6919102e-06]\n",
      "Epoch: 3200  | Total loss: [1.3212091e-06]\n",
      "Epoch: 3300  | Total loss: [0.00188452]\n",
      "Epoch: 3400  | Total loss: [1.0851518e-06]\n",
      "Epoch: 3500  | Total loss: [1.5633292e-09]\n",
      "Epoch: 3600  | Total loss: [0.00011865]\n",
      "Epoch: 3700  | Total loss: [9.1698936e-08]\n",
      "Epoch: 3800  | Total loss: [5.0111373e-09]\n",
      "Epoch: 3900  | Total loss: [9.0197894e-08]\n",
      "Epoch: 4000  | Total loss: [1.1557567]\n",
      "Epoch: 4100  | Total loss: [0.00026934]\n",
      "Epoch: 4200  | Total loss: [3.5932637e-05]\n",
      "Epoch: 4300  | Total loss: [2.1664644e-05]\n",
      "Epoch: 4400  | Total loss: [1.4452249e-05]\n",
      "Epoch: 4500  | Total loss: [1.0236957e-05]\n",
      "Epoch: 4600  | Total loss: [7.5427047e-06]\n",
      "Epoch: 4700  | Total loss: [5.713053e-06]\n",
      "Epoch: 4800  | Total loss: [4.4153494e-06]\n",
      "Epoch: 4900  | Total loss: [3.4650127e-06]\n",
      "Epoch: 5000  | Total loss: [2.752544e-06]\n",
      "Epoch: 5100  | Total loss: [2.2080749e-06]\n",
      "Epoch: 5200  | Total loss: [1.7870731e-06]\n",
      "Epoch: 5300  | Total loss: [1.4586014e-06]\n",
      "Epoch: 5400  | Total loss: [1.3314254e-06]\n",
      "Epoch: 5500  | Total loss: [1.2696387e-06]\n",
      "Epoch: 5600  | Total loss: [9.649043e-07]\n",
      "Epoch: 5700  | Total loss: [8.013985e-07]\n",
      "Epoch: 5800  | Total loss: [6.7749664e-07]\n",
      "Epoch: 5900  | Total loss: [0.00247618]\n",
      "Epoch: 6000  | Total loss: [5.6229646e-07]\n",
      "Epoch: 6100  | Total loss: [4.7927585e-07]\n",
      "Epoch: 6200  | Total loss: [4.2448738e-07]\n",
      "Epoch: 6300  | Total loss: [0.00472694]\n",
      "Epoch: 6400  | Total loss: [6.1716696e-07]\n",
      "Epoch: 6500  | Total loss: [3.12699e-07]\n",
      "Epoch: 6600  | Total loss: [4.908173e-05]\n",
      "Epoch: 6700  | Total loss: [3.425863e-07]\n",
      "Epoch: 6800  | Total loss: [0.00120568]\n",
      "Epoch: 6900  | Total loss: [0.00025053]\n",
      "Epoch: 7000  | Total loss: [3.7680286e-07]\n",
      "Epoch: 7100  | Total loss: [7.1093405e-07]\n",
      "Epoch: 7200  | Total loss: [1.4253457e-05]\n",
      "Epoch: 7300  | Total loss: [4.181714e-07]\n",
      "Epoch: 7400  | Total loss: [0.69752234]\n",
      "Epoch: 7500  | Total loss: [2.825792e-05]\n",
      "Epoch: 7600  | Total loss: [6.209073e-07]\n",
      "Epoch: 7700  | Total loss: [4.945872e-07]\n",
      "Epoch: 7800  | Total loss: [3.960321e-07]\n",
      "Epoch: 7900  | Total loss: [3.183788e-07]\n",
      "Epoch: 8000  | Total loss: [2.5757325e-07]\n",
      "Epoch: 8100  | Total loss: [2.1018836e-07]\n",
      "Epoch: 8200  | Total loss: [1.7350783e-07]\n",
      "Epoch: 8300  | Total loss: [1.451809e-07]\n",
      "Epoch: 8400  | Total loss: [1.2351886e-07]\n",
      "Epoch: 8500  | Total loss: [1.07053964e-07]\n",
      "Epoch: 8600  | Total loss: [9.463477e-08]\n",
      "Epoch: 8700  | Total loss: [8.515843e-08]\n",
      "Epoch: 8800  | Total loss: [7.806939e-08]\n",
      "Epoch: 8900  | Total loss: [7.267623e-08]\n",
      "Epoch: 9000  | Total loss: [6.8520805e-08]\n",
      "Epoch: 9100  | Total loss: [6.522835e-08]\n",
      "Epoch: 9200  | Total loss: [6.255489e-08]\n",
      "Epoch: 9300  | Total loss: [0.00026281]\n",
      "Epoch: 9400  | Total loss: [7.093169e-08]\n",
      "Epoch: 9500  | Total loss: [5.7592764e-08]\n",
      "Epoch: 9600  | Total loss: [0.00031183]\n",
      "Epoch: 9700  | Total loss: [1.0766905e-07]\n",
      "Epoch: 9800  | Total loss: [6.260359e-08]\n",
      "Epoch: 9900  | Total loss: [0.00645842]\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "epochs = 10000\n",
    "epoch_count = []\n",
    "loss_track = []\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Putting the model in training mode\n",
    "    my_first_model.train()\n",
    "\n",
    "    y_preds = my_first_model(x_input)\n",
    "    # Calculating the derivative d2y_dx2\n",
    "    dy_dx = torch.autograd.grad(outputs=y_preds, inputs= x_input, grad_outputs= torch.ones_like(y_preds), create_graph= True)[0]\n",
    "    # and finally\n",
    "    d2y_dx2 = torch.autograd.grad(outputs= dy_dx, inputs= x_input, grad_outputs=torch.ones_like(dy_dx), create_graph=True)[0]\n",
    "    # Caluclating the y at the boundary value\n",
    "\n",
    "    y_pred_bc = my_first_model(x_bc)\n",
    "    # SO the loss is to be define here then\n",
    "    loss_diff_eq = torch.mean((d2y_dx2+5*y_preds)**2) \n",
    "    loss_bc = (y_pred_bc-y_bc)**2\n",
    "\n",
    "    total_loss = loss_diff_eq + loss_bc\n",
    "\n",
    "\n",
    "    # Setting the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculating the backward pass\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Updating the model\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100==0:\n",
    "        epoch_count.append(i)\n",
    "        loss_track.append(total_loss.detach().numpy())\n",
    "        print(f\"Epoch: {i}  | Total loss: {total_loss.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8859f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9702, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.9702431, dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_loss)\n",
    "total_loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6815c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_model.train()\n",
    "\n",
    "y_preds = my_first_model(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e7a694c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = torch.autograd.grad(outputs=y_preds, inputs= x_input, grad_outputs= torch.ones_like(y_preds), create_graph= True)[0]\n",
    "    # and finally\n",
    "d2y_dx2 = torch.autograd.grad(outputs= dy_dx, inputs= x_input, grad_outputs=torch.ones_like(dy_dx), create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d60ebb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9702, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_diff_eq = torch.mean((d2y_dx2+5*y_preds)**2) \n",
    "y_pred_bc = my_first_model(x_bc)\n",
    "loss_bc = torch.mean((y_pred_bc-y_bc)**2)\n",
    "\n",
    "total_loss = loss_diff_eq + loss_bc\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c505d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cdc1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b206c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6e6b1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0344],\n",
       "        [0.0166],\n",
       "        [0.0164],\n",
       "        [0.0191],\n",
       "        [0.0209],\n",
       "        [0.0221],\n",
       "        [0.0229],\n",
       "        [0.0234],\n",
       "        [0.0237],\n",
       "        [0.0238]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d2y_dx2+5*y_preds)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb9f54b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9242], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred_bc-y_bc)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d27b704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input_layer.weight',\n",
       "              tensor([[ 0.7645],\n",
       "                      [ 0.8300],\n",
       "                      [-0.2343],\n",
       "                      [ 0.9186],\n",
       "                      [-0.2191],\n",
       "                      [ 0.2018],\n",
       "                      [-0.4869],\n",
       "                      [ 0.5873],\n",
       "                      [ 0.8815],\n",
       "                      [-0.7336],\n",
       "                      [ 0.8692],\n",
       "                      [ 0.1872],\n",
       "                      [ 0.7388],\n",
       "                      [ 0.1354],\n",
       "                      [ 0.4822],\n",
       "                      [-0.1412],\n",
       "                      [ 0.7709],\n",
       "                      [ 0.1478],\n",
       "                      [-0.4668],\n",
       "                      [ 0.2549]])),\n",
       "             ('input_layer.bias',\n",
       "              tensor([-0.4607, -0.1173, -0.4062,  0.6634, -0.7894, -0.4610, -0.2824, -0.6013,\n",
       "                       0.0944, -0.9877,  0.9031, -0.8495,  0.7720,  0.1664, -0.3247,  0.6179,\n",
       "                       0.1559,  0.8080,  0.1093, -0.3154])),\n",
       "             ('hidden_stack.0.weight',\n",
       "              tensor([[ 0.0601, -0.0606,  0.0941,  0.1996,  0.1293, -0.0978,  0.1291,  0.0400,\n",
       "                        0.1136, -0.1363, -0.2213, -0.0864, -0.1715,  0.1835,  0.0644,  0.0926,\n",
       "                        0.0707, -0.0039,  0.1750, -0.1589],\n",
       "                      [ 0.0141, -0.1526,  0.0689, -0.0770,  0.0685, -0.0466,  0.1855, -0.1325,\n",
       "                       -0.1334, -0.1334,  0.2011,  0.0745,  0.2152, -0.1845, -0.2218, -0.1749,\n",
       "                       -0.1504,  0.0906,  0.0801,  0.1858],\n",
       "                      [-0.1155, -0.1524,  0.1186, -0.0904,  0.1357, -0.0531,  0.1279, -0.1737,\n",
       "                       -0.1128,  0.0682,  0.0473, -0.0570,  0.1333,  0.1520, -0.1622, -0.1194,\n",
       "                        0.2047, -0.0755, -0.0793, -0.2164],\n",
       "                      [-0.1281,  0.0559, -0.0295, -0.1623,  0.0052, -0.1527, -0.1897, -0.1231,\n",
       "                       -0.1957, -0.1424,  0.2235,  0.0422,  0.0689, -0.2086, -0.1469, -0.0744,\n",
       "                        0.0350, -0.1968, -0.0963, -0.1339],\n",
       "                      [ 0.0006, -0.0832, -0.0155, -0.1515, -0.1535, -0.1305, -0.0765, -0.1765,\n",
       "                        0.1875, -0.0444,  0.1924,  0.0697, -0.1893,  0.1547, -0.0615, -0.0857,\n",
       "                       -0.1856, -0.2223,  0.0640, -0.0488],\n",
       "                      [ 0.0871, -0.1835,  0.1660, -0.1641, -0.0386,  0.0467,  0.1154,  0.1805,\n",
       "                        0.2037, -0.1773,  0.0563, -0.0962, -0.0245, -0.1674,  0.2037, -0.1641,\n",
       "                        0.1195,  0.0786,  0.0727, -0.1209],\n",
       "                      [ 0.2033,  0.0491,  0.0288, -0.1971,  0.0939, -0.0335, -0.1024,  0.1921,\n",
       "                        0.0499, -0.1237, -0.1132, -0.0107,  0.1249, -0.0571, -0.1276, -0.0766,\n",
       "                       -0.1670,  0.0797,  0.1731, -0.2105],\n",
       "                      [ 0.0519,  0.1155,  0.0405, -0.0796,  0.1167,  0.1175,  0.0836, -0.0393,\n",
       "                       -0.0592,  0.0239, -0.0395, -0.0666,  0.1429,  0.1922, -0.0221, -0.0501,\n",
       "                        0.0033, -0.0134,  0.0538,  0.0627],\n",
       "                      [-0.2031, -0.0825,  0.1883,  0.0871, -0.0111, -0.1348, -0.1368, -0.2003,\n",
       "                       -0.0729,  0.0755,  0.1426,  0.1032, -0.1977, -0.1345, -0.0353,  0.2163,\n",
       "                        0.0323, -0.0579,  0.0925, -0.0852],\n",
       "                      [-0.1447,  0.1632, -0.1017, -0.0448, -0.2224,  0.1497,  0.1694,  0.0815,\n",
       "                       -0.1559, -0.2207, -0.1816,  0.1667,  0.1074,  0.1882,  0.1171,  0.0566,\n",
       "                       -0.0022, -0.1701, -0.1916, -0.2091],\n",
       "                      [ 0.0915, -0.1098, -0.0450, -0.1287, -0.0407, -0.1574, -0.1461,  0.0742,\n",
       "                       -0.0665,  0.1380, -0.0717, -0.1640, -0.0395, -0.1084, -0.0684, -0.2129,\n",
       "                        0.1251, -0.1557,  0.1124,  0.1015],\n",
       "                      [ 0.1598, -0.1715,  0.1608, -0.1057,  0.0830,  0.2100, -0.0315, -0.0017,\n",
       "                       -0.0515, -0.1867,  0.1073, -0.2220,  0.1388,  0.1673,  0.2115, -0.0527,\n",
       "                       -0.1837,  0.0503,  0.1235, -0.2226],\n",
       "                      [-0.0508, -0.1340, -0.0196, -0.1101, -0.0914, -0.0710, -0.2125,  0.1835,\n",
       "                        0.1875, -0.0351, -0.0255, -0.0913, -0.2019, -0.2176,  0.0831, -0.1228,\n",
       "                       -0.1438, -0.0174, -0.0745, -0.0723],\n",
       "                      [ 0.0072, -0.0474, -0.0770, -0.1071, -0.1820,  0.1875, -0.0895,  0.0592,\n",
       "                       -0.0776,  0.0182,  0.2085,  0.1030, -0.1938,  0.0887,  0.2123,  0.0588,\n",
       "                        0.1499,  0.2205, -0.0343,  0.0464],\n",
       "                      [-0.1554, -0.0461,  0.1656,  0.1146, -0.1415, -0.1793, -0.1528, -0.2207,\n",
       "                       -0.1725, -0.0553,  0.1509,  0.0374, -0.1701, -0.1794,  0.1112, -0.1663,\n",
       "                       -0.0275,  0.1073, -0.1035, -0.0244],\n",
       "                      [-0.0195, -0.0529, -0.1134, -0.1993, -0.1808, -0.1197,  0.2160, -0.1080,\n",
       "                       -0.1502,  0.0542,  0.0616,  0.1225,  0.1700,  0.1245, -0.2217,  0.0198,\n",
       "                        0.1354, -0.0207, -0.1318,  0.2132],\n",
       "                      [-0.0836, -0.1273, -0.2016,  0.0100,  0.0991,  0.0495,  0.0442, -0.1696,\n",
       "                       -0.2088,  0.0039,  0.2039,  0.1290, -0.1302, -0.0290, -0.1648, -0.1079,\n",
       "                        0.0405,  0.1218,  0.1852, -0.2053],\n",
       "                      [ 0.1495, -0.1577,  0.0837,  0.1892,  0.0031,  0.2034, -0.1905, -0.0854,\n",
       "                        0.1304, -0.0487, -0.0458, -0.0932,  0.1541,  0.1097,  0.0717, -0.1257,\n",
       "                       -0.1815,  0.0242,  0.0663, -0.1032],\n",
       "                      [-0.0626,  0.1510,  0.0178,  0.0101, -0.0550, -0.2025, -0.2102, -0.1069,\n",
       "                       -0.1137,  0.0697, -0.0651, -0.0875,  0.2132,  0.0779,  0.1594, -0.1082,\n",
       "                       -0.0913,  0.0822, -0.1490, -0.1462],\n",
       "                      [-0.0108, -0.0818, -0.1676,  0.1326,  0.1798,  0.0363, -0.0389, -0.2071,\n",
       "                       -0.0814,  0.0569,  0.1054, -0.0283, -0.0884,  0.1246, -0.1781,  0.1413,\n",
       "                       -0.0868,  0.0034, -0.0442,  0.0271]])),\n",
       "             ('hidden_stack.0.bias',\n",
       "              tensor([-0.0676,  0.1626, -0.0058,  0.1745,  0.2150, -0.1089, -0.1631,  0.1794,\n",
       "                       0.1752, -0.1707, -0.0173, -0.2205, -0.1830,  0.0432,  0.0595,  0.0474,\n",
       "                      -0.0609,  0.2063,  0.0320, -0.1319])),\n",
       "             ('hidden_stack.2.weight',\n",
       "              tensor([[-0.0127,  0.0537,  0.0783, -0.1581,  0.0838, -0.1142, -0.1858, -0.1221,\n",
       "                        0.2157,  0.1912,  0.2002,  0.1313,  0.1689, -0.0299, -0.1230,  0.1117,\n",
       "                       -0.1159, -0.1509, -0.0714,  0.0469],\n",
       "                      [ 0.1151, -0.0869, -0.1316,  0.0302, -0.1318, -0.1456,  0.1166, -0.0376,\n",
       "                        0.2043,  0.2175,  0.0669,  0.0770,  0.0515,  0.0035, -0.0163,  0.0031,\n",
       "                        0.0835,  0.2079, -0.0580, -0.0945],\n",
       "                      [-0.0542, -0.1080,  0.0380,  0.1669,  0.1749,  0.1027, -0.1646, -0.1200,\n",
       "                       -0.0491, -0.0412,  0.0184, -0.2053,  0.0696, -0.1706, -0.1415, -0.1859,\n",
       "                        0.1948, -0.2117,  0.1687, -0.0075],\n",
       "                      [-0.0260,  0.1399, -0.0207,  0.1402,  0.1617, -0.1941,  0.0860,  0.0422,\n",
       "                        0.0481,  0.0326,  0.0612, -0.1076, -0.0286,  0.2125,  0.1502, -0.0084,\n",
       "                       -0.2103,  0.0098, -0.1523,  0.1818],\n",
       "                      [-0.1358, -0.0161, -0.0496,  0.0398,  0.2104,  0.0212,  0.1295,  0.1736,\n",
       "                        0.1805, -0.0772, -0.0500,  0.1078, -0.0610,  0.1047, -0.0489, -0.1517,\n",
       "                        0.0910,  0.0343,  0.0997,  0.2221],\n",
       "                      [ 0.1527,  0.2120,  0.0120, -0.1924, -0.1569, -0.1389, -0.1971, -0.1121,\n",
       "                       -0.2058, -0.2063, -0.1336, -0.2204, -0.1373,  0.0853,  0.1865, -0.0665,\n",
       "                       -0.0650,  0.1194, -0.1103, -0.1057],\n",
       "                      [ 0.1378, -0.1948,  0.0273,  0.1975,  0.0383,  0.0608, -0.1302, -0.0031,\n",
       "                        0.0123,  0.0549,  0.0869,  0.1943, -0.1707,  0.0067, -0.1117, -0.1769,\n",
       "                       -0.0179, -0.1968,  0.1561,  0.0259],\n",
       "                      [-0.1205,  0.1168, -0.2116, -0.0865, -0.0436, -0.1900, -0.1422, -0.0365,\n",
       "                        0.1697,  0.2159,  0.1423, -0.1335, -0.1463,  0.1951,  0.0791,  0.0059,\n",
       "                        0.0303, -0.1797, -0.0747,  0.2152],\n",
       "                      [-0.0551, -0.0112, -0.1857, -0.1251, -0.0046, -0.1389, -0.0277,  0.0910,\n",
       "                       -0.2187,  0.0664, -0.1479, -0.1091,  0.0858,  0.1778, -0.0611, -0.0918,\n",
       "                       -0.2022, -0.1153, -0.1958, -0.0512],\n",
       "                      [ 0.0456, -0.2095,  0.1952,  0.1403, -0.2189, -0.1068,  0.0729, -0.0459,\n",
       "                       -0.0244, -0.1010,  0.1796, -0.1250,  0.1854,  0.0144,  0.0450,  0.1744,\n",
       "                       -0.0368, -0.1273, -0.0362,  0.1814],\n",
       "                      [-0.1659,  0.0507, -0.2198,  0.1172,  0.0826,  0.0095,  0.0960,  0.0003,\n",
       "                        0.1237, -0.1770, -0.0328,  0.0992,  0.2227,  0.1139, -0.1626,  0.1720,\n",
       "                       -0.0499, -0.0477, -0.2032, -0.0352],\n",
       "                      [ 0.1582,  0.0312, -0.1302,  0.0688, -0.0717,  0.2041, -0.1941, -0.0706,\n",
       "                       -0.2159, -0.0881,  0.0705,  0.2152,  0.0376,  0.2192,  0.0438,  0.1291,\n",
       "                        0.1793,  0.1869, -0.1252,  0.2056],\n",
       "                      [ 0.1355, -0.1046, -0.1067, -0.1875,  0.0562, -0.1812,  0.0945,  0.0706,\n",
       "                       -0.1943,  0.0609, -0.0182,  0.1021,  0.1283, -0.2223,  0.2051,  0.1875,\n",
       "                        0.0890, -0.2044, -0.0799, -0.0648],\n",
       "                      [-0.0575,  0.1261,  0.0813,  0.1771, -0.0837,  0.0753,  0.0796, -0.1862,\n",
       "                       -0.2169, -0.1160,  0.1531, -0.2105, -0.1946,  0.1253,  0.1206,  0.1839,\n",
       "                       -0.1688, -0.1637,  0.1147,  0.1945],\n",
       "                      [ 0.1338,  0.0350,  0.0737,  0.2122, -0.1443, -0.1015,  0.1564, -0.1530,\n",
       "                       -0.1233,  0.1632,  0.0706,  0.0722, -0.0948, -0.0031,  0.2047, -0.1342,\n",
       "                        0.0018,  0.1063, -0.1544,  0.2172],\n",
       "                      [-0.1117, -0.0537, -0.0605, -0.1457, -0.2194,  0.1261,  0.0594, -0.2094,\n",
       "                       -0.1439,  0.2210,  0.0855,  0.0897, -0.1338, -0.0980, -0.0338, -0.0409,\n",
       "                       -0.1532,  0.0184,  0.0222, -0.0283],\n",
       "                      [ 0.0310, -0.0886,  0.0582,  0.0843, -0.1178, -0.2217,  0.1170,  0.0533,\n",
       "                       -0.1137,  0.2155, -0.1011,  0.1511,  0.1134, -0.1875,  0.1442, -0.2056,\n",
       "                       -0.1239, -0.0373, -0.1507,  0.2184],\n",
       "                      [-0.0449,  0.0888, -0.1997,  0.1287, -0.0695, -0.1701,  0.0327,  0.1083,\n",
       "                        0.1935, -0.1366, -0.1100,  0.0430,  0.0607,  0.0860,  0.1227, -0.0507,\n",
       "                        0.1242,  0.1649, -0.0584,  0.1591],\n",
       "                      [ 0.1092,  0.1972, -0.1271, -0.1104, -0.0647,  0.0113,  0.1342, -0.1276,\n",
       "                        0.1119, -0.0801,  0.1351, -0.0106, -0.1959, -0.1230, -0.1618,  0.1109,\n",
       "                       -0.1499, -0.0186,  0.0482, -0.1226],\n",
       "                      [ 0.0645, -0.2183, -0.1600, -0.2026, -0.0676, -0.0815,  0.0320, -0.0413,\n",
       "                        0.1051,  0.2050,  0.0802, -0.0881, -0.2094,  0.0810, -0.1108,  0.1138,\n",
       "                        0.1495,  0.0862,  0.2098,  0.2124]])),\n",
       "             ('hidden_stack.2.bias',\n",
       "              tensor([ 0.0473, -0.1629,  0.1998, -0.1061, -0.1056,  0.1871,  0.1733,  0.0676,\n",
       "                       0.0140, -0.1881, -0.0232,  0.2145,  0.0569,  0.0191, -0.0464, -0.0780,\n",
       "                       0.1333,  0.0138,  0.1455, -0.0396])),\n",
       "             ('hidden_stack.4.weight',\n",
       "              tensor([[ 9.7695e-02,  9.2298e-02,  3.5656e-02,  1.4051e-01,  1.4012e-01,\n",
       "                        2.0727e-01,  1.7190e-01, -5.7176e-02, -1.8931e-01,  4.0879e-02,\n",
       "                       -1.9514e-03, -5.8322e-02, -3.7444e-02,  1.0516e-02,  1.6315e-01,\n",
       "                        6.9710e-02, -7.9402e-02, -9.1954e-02, -5.5372e-02, -8.6424e-02],\n",
       "                      [ 2.0107e-01,  1.1843e-01,  2.0191e-01,  7.1566e-04,  4.5097e-02,\n",
       "                        7.7538e-02, -2.1165e-01,  1.9968e-02, -1.4963e-02, -1.2537e-01,\n",
       "                       -1.7351e-01,  1.9795e-01,  1.8181e-01,  1.0364e-01,  2.1338e-01,\n",
       "                       -9.0747e-02, -3.8622e-02,  8.4665e-02, -3.6945e-02, -4.3881e-02],\n",
       "                      [-1.8482e-01,  6.0074e-02, -1.3513e-01,  8.1332e-03,  2.1801e-01,\n",
       "                       -6.8830e-02, -7.0478e-02,  1.3491e-01, -8.2216e-02, -1.9202e-02,\n",
       "                        2.0879e-01, -9.1673e-02, -1.5997e-01, -1.2514e-01, -6.1992e-02,\n",
       "                       -1.0611e-01, -1.1604e-01,  9.0328e-02,  3.8000e-02, -7.1569e-02],\n",
       "                      [-1.7372e-01, -7.0406e-02, -9.4366e-02, -7.2762e-02, -2.0172e-01,\n",
       "                        4.8170e-02, -1.6429e-01, -1.7414e-01, -1.8271e-01,  9.3331e-02,\n",
       "                       -1.3462e-01, -9.2294e-02,  1.7527e-01,  1.1873e-01,  1.2822e-01,\n",
       "                       -2.1233e-01, -1.6035e-01, -8.4406e-02,  1.8472e-01,  2.2872e-02],\n",
       "                      [-1.6723e-01,  1.4001e-03, -1.7367e-01, -4.8992e-02, -6.1486e-02,\n",
       "                        1.9357e-01,  6.9259e-02, -3.8990e-02,  3.7773e-02, -6.4548e-02,\n",
       "                        8.7855e-02,  8.8451e-02,  6.0049e-02, -8.7157e-02,  1.9077e-01,\n",
       "                       -3.2283e-02, -8.7054e-02,  1.4005e-01,  1.8225e-01,  2.2252e-01],\n",
       "                      [ 6.6247e-02, -7.6227e-02,  1.1357e-01,  1.9185e-01, -2.1932e-01,\n",
       "                       -2.7703e-02, -1.5249e-01,  4.1672e-02,  9.2472e-02, -4.6193e-02,\n",
       "                       -1.8707e-02,  1.0065e-01, -3.7582e-02, -1.8778e-01,  1.7892e-01,\n",
       "                       -1.1255e-01, -2.4565e-02,  2.1097e-02, -1.3438e-02, -2.1035e-01],\n",
       "                      [ 1.0259e-01, -1.0157e-01, -1.1597e-01,  5.3424e-02, -1.1669e-01,\n",
       "                       -1.0334e-01, -7.5344e-02, -8.4004e-02, -9.3390e-02, -6.0303e-02,\n",
       "                        5.8110e-02, -1.8095e-01, -1.3535e-01,  3.2632e-03,  3.1098e-02,\n",
       "                        1.2349e-01, -1.5707e-01,  7.1377e-02,  1.2710e-01,  1.2416e-01],\n",
       "                      [-2.0827e-01, -8.5329e-02, -1.9221e-01, -1.4150e-01,  1.2454e-01,\n",
       "                       -3.3388e-02,  9.4968e-02, -1.3126e-01,  3.3982e-02, -1.3525e-01,\n",
       "                        1.1178e-01, -9.7801e-02, -5.6071e-02, -1.9401e-01,  7.3772e-04,\n",
       "                        2.1231e-01,  1.0853e-01, -1.1930e-01,  3.0071e-03, -2.4517e-02],\n",
       "                      [-1.8002e-01,  1.7533e-01,  3.6023e-03,  4.7089e-02, -9.0301e-02,\n",
       "                       -1.0463e-01,  3.6869e-02,  8.2669e-02,  5.0161e-02, -1.0777e-01,\n",
       "                        2.1710e-01, -3.2916e-02, -1.3694e-01, -1.0458e-01,  2.2012e-01,\n",
       "                        1.6251e-05, -3.0354e-02, -9.3060e-02, -5.8618e-02, -1.8833e-01],\n",
       "                      [-1.7770e-01,  1.3087e-01,  1.9128e-01,  2.1339e-01, -1.6143e-01,\n",
       "                        1.2094e-01, -1.3841e-01,  1.3339e-01,  1.6134e-01,  1.7304e-01,\n",
       "                        1.6101e-01,  1.3988e-01,  4.3521e-03,  1.0274e-01, -8.0000e-02,\n",
       "                        9.7366e-02, -7.1868e-02, -3.7543e-03, -1.9462e-01, -5.8468e-02],\n",
       "                      [-1.1759e-01, -7.5428e-02, -1.4279e-01, -2.0112e-01,  1.4563e-02,\n",
       "                        1.4512e-01,  2.0365e-01,  1.3049e-01, -1.1591e-01, -2.2115e-01,\n",
       "                        8.4817e-02,  1.2529e-01, -1.9197e-01,  8.0183e-02,  1.8906e-01,\n",
       "                        1.3541e-02, -1.3471e-01,  1.8333e-01,  9.5484e-02,  1.4807e-01],\n",
       "                      [-1.5122e-01,  1.3013e-01, -1.5274e-01,  2.2126e-01, -9.4740e-02,\n",
       "                        1.3474e-01,  4.4776e-02,  5.9259e-02, -3.4296e-02,  9.1844e-02,\n",
       "                       -9.3194e-02, -2.1077e-01, -8.5911e-02,  1.7521e-01, -5.8867e-02,\n",
       "                        7.0280e-02, -8.2673e-02,  1.6774e-01,  1.3383e-01,  7.8935e-02],\n",
       "                      [-1.1430e-01, -1.8272e-01,  8.4191e-03, -1.3118e-01,  1.8385e-01,\n",
       "                       -2.1488e-01,  9.9923e-02,  2.2292e-01,  1.1199e-01,  7.6245e-02,\n",
       "                       -2.1514e-01,  2.1509e-01, -3.8248e-02, -2.0894e-01,  2.2074e-01,\n",
       "                       -9.0994e-02, -1.5820e-02,  2.0466e-01, -1.5500e-01, -1.5820e-01],\n",
       "                      [ 3.6364e-02, -2.9931e-02,  5.1510e-02, -1.8754e-01,  6.6915e-03,\n",
       "                       -9.9452e-02, -1.0993e-01, -2.0474e-01,  1.1855e-01,  4.3076e-02,\n",
       "                       -1.8905e-01,  1.7744e-01,  6.7443e-02,  4.1495e-02, -1.3128e-01,\n",
       "                        3.3712e-02,  2.1545e-01,  1.5337e-01, -1.7416e-01,  2.0411e-01],\n",
       "                      [ 1.7328e-02,  1.0755e-01,  1.7366e-01,  1.9062e-01, -1.7432e-01,\n",
       "                        1.9581e-01, -1.5187e-01,  1.6761e-02, -1.5623e-01, -4.9025e-02,\n",
       "                       -1.0160e-02, -2.6750e-02, -3.5323e-02,  1.7634e-02,  2.2056e-01,\n",
       "                        1.2993e-01,  1.2510e-01,  8.9489e-02,  1.7311e-01, -1.0315e-02],\n",
       "                      [ 1.7779e-02,  4.6018e-02, -1.9502e-01, -1.8013e-01,  2.7414e-02,\n",
       "                       -8.7486e-02, -4.1034e-03, -5.1309e-02,  3.4809e-02,  1.4548e-01,\n",
       "                       -7.4157e-02,  1.7908e-01,  1.7655e-01, -1.7162e-01, -1.7268e-01,\n",
       "                       -1.8091e-01, -1.2254e-01, -8.7046e-02, -1.6831e-02, -5.4379e-02],\n",
       "                      [-1.1298e-01, -7.1038e-02, -8.0889e-02,  2.1937e-01, -8.2876e-02,\n",
       "                       -1.6010e-01,  9.2952e-02, -1.2916e-02,  1.7119e-01,  1.3972e-01,\n",
       "                        2.0543e-01, -1.6376e-01,  1.4375e-01,  1.8766e-01, -1.1043e-01,\n",
       "                        2.0554e-01,  1.6763e-01,  2.4473e-03,  1.0781e-01, -7.8177e-02],\n",
       "                      [-1.9503e-01,  5.6524e-02,  6.6662e-02, -1.4614e-01,  1.0845e-01,\n",
       "                       -1.9101e-01,  1.9244e-01,  2.1654e-01,  6.0882e-02, -1.4030e-01,\n",
       "                        1.0882e-01,  3.8106e-02,  6.0803e-02,  7.3489e-02,  1.7024e-01,\n",
       "                       -9.6112e-02, -5.0299e-02,  6.0977e-02,  2.4364e-02,  1.8031e-01],\n",
       "                      [-1.1745e-01, -8.1417e-03,  4.1775e-02, -5.9392e-02,  1.5243e-01,\n",
       "                        2.4460e-02, -2.0667e-01, -2.4227e-02, -1.0140e-01,  2.1726e-02,\n",
       "                       -2.5985e-02, -2.2180e-01, -4.0745e-02, -2.1419e-02, -6.5942e-02,\n",
       "                        2.0546e-01, -4.8792e-02,  1.4364e-01,  5.5391e-02, -1.8875e-01],\n",
       "                      [ 5.2545e-02,  1.8531e-01, -1.4626e-01, -1.4453e-01,  2.1888e-01,\n",
       "                        1.7967e-01, -1.2475e-01,  1.3455e-01, -2.8607e-02, -2.2046e-01,\n",
       "                        1.6800e-02,  7.2246e-02, -6.7073e-02,  7.7789e-02, -1.9121e-01,\n",
       "                        1.5496e-01,  1.9063e-01,  1.2331e-01,  3.7899e-02,  7.3645e-02]])),\n",
       "             ('hidden_stack.4.bias',\n",
       "              tensor([-0.1618, -0.0559, -0.0213, -0.1244, -0.1652,  0.1504,  0.1518, -0.2031,\n",
       "                       0.0712,  0.0910,  0.2124,  0.1294,  0.2056, -0.0732,  0.1566,  0.1819,\n",
       "                      -0.2112, -0.1795,  0.0453,  0.1221])),\n",
       "             ('hidden_stack.6.weight',\n",
       "              tensor([[-1.1109e-01,  2.4627e-02, -3.2426e-04,  5.6597e-02, -1.2016e-01,\n",
       "                        1.2612e-01,  1.4870e-01, -1.5515e-01,  2.1661e-03,  6.2001e-04,\n",
       "                        7.1804e-02,  2.0831e-01,  6.5576e-02, -5.0809e-02,  2.0083e-01,\n",
       "                       -8.5090e-02, -6.4930e-02,  1.5267e-02, -1.7028e-01,  2.4332e-02],\n",
       "                      [-1.5171e-01,  2.2996e-02,  2.1426e-02,  3.0954e-02, -1.8857e-01,\n",
       "                       -2.1238e-01,  1.0289e-01,  1.9176e-01, -1.9842e-01,  8.2819e-02,\n",
       "                       -1.6460e-01,  1.4019e-02,  2.0804e-01,  1.6963e-01, -1.6348e-01,\n",
       "                        1.3832e-01,  1.1679e-01, -3.4257e-04,  2.1661e-01, -8.8813e-02],\n",
       "                      [-5.2042e-02, -1.1302e-01,  3.2172e-02,  5.9205e-02,  1.7821e-02,\n",
       "                       -1.2581e-01, -1.0066e-01,  7.6071e-02, -1.9606e-01, -3.4818e-02,\n",
       "                       -4.9576e-03, -9.0599e-02,  8.1367e-02,  1.4417e-02,  1.5698e-01,\n",
       "                       -1.9858e-02, -1.4112e-01,  1.5799e-01,  3.1225e-02, -6.4038e-02],\n",
       "                      [-1.4738e-01,  5.2405e-02, -2.1327e-01,  5.8890e-02,  1.1056e-01,\n",
       "                       -1.9615e-01, -1.4639e-01, -1.6075e-01,  1.8794e-01,  1.9010e-01,\n",
       "                        1.3994e-02,  2.0992e-01, -7.4760e-02, -2.1708e-01,  7.8099e-02,\n",
       "                        7.9218e-02,  2.1923e-01, -6.5289e-05,  1.7137e-01,  1.3706e-01],\n",
       "                      [ 1.6184e-01, -2.2190e-01,  1.8413e-01, -1.9062e-01,  2.0047e-01,\n",
       "                       -9.2685e-03, -1.9816e-01,  6.2190e-02, -4.5514e-02, -2.0084e-01,\n",
       "                       -8.1069e-02,  2.3054e-02,  1.7415e-01, -6.8741e-02,  1.3281e-01,\n",
       "                        2.8002e-02,  7.5424e-03,  4.2786e-02, -1.4675e-01, -1.8586e-01],\n",
       "                      [-7.0257e-02, -2.0836e-01, -1.0428e-01,  1.0289e-02, -1.4727e-01,\n",
       "                       -9.2749e-02, -4.0179e-02, -2.5768e-02, -1.2343e-01,  1.0741e-02,\n",
       "                       -3.1382e-02,  6.3952e-02,  1.0105e-02, -1.1167e-01, -7.6980e-02,\n",
       "                       -2.5778e-03, -1.3953e-01,  1.7505e-02, -1.5787e-02,  8.6753e-02],\n",
       "                      [ 5.2923e-02,  1.6735e-01, -7.3680e-03, -1.6522e-01,  9.9951e-02,\n",
       "                       -1.5112e-01, -3.0735e-02, -7.2412e-02, -8.4847e-02, -8.2735e-02,\n",
       "                        2.0548e-01,  1.2801e-01,  6.1976e-02,  2.0995e-01, -2.0263e-01,\n",
       "                        3.6082e-02,  1.4314e-01,  1.4854e-01, -1.7796e-01,  7.9578e-02],\n",
       "                      [-1.2739e-01, -8.7420e-03, -4.6664e-02, -1.8671e-01, -2.1330e-01,\n",
       "                       -1.4711e-01,  1.0147e-01,  1.0229e-01, -1.9448e-01, -8.3970e-02,\n",
       "                        3.6490e-03, -1.9333e-01,  1.2599e-01,  2.0253e-01,  3.9543e-02,\n",
       "                       -1.6614e-01,  5.2149e-02,  8.1172e-02,  1.7706e-01, -1.1897e-01],\n",
       "                      [-2.0033e-01,  9.8243e-02,  1.3922e-02, -7.2856e-02, -9.3697e-02,\n",
       "                       -1.1529e-01,  1.8101e-01, -3.8583e-02,  1.6127e-01,  1.9960e-01,\n",
       "                        2.8327e-02,  4.7633e-02, -1.5227e-02,  1.5358e-01, -2.0149e-01,\n",
       "                        8.8924e-02, -3.6434e-02,  1.0210e-01, -9.9563e-04, -8.2794e-02],\n",
       "                      [-2.4141e-02,  1.2074e-01, -1.5530e-01, -1.4134e-01, -5.0085e-02,\n",
       "                       -8.1110e-02,  8.7841e-02,  1.5049e-01, -2.3947e-02,  2.1144e-01,\n",
       "                       -9.3430e-02, -1.2548e-01, -1.0011e-01, -1.5406e-01, -1.5674e-01,\n",
       "                       -1.9924e-01, -1.9817e-01,  5.8361e-02,  1.3637e-01,  2.0993e-01],\n",
       "                      [ 1.9047e-01, -1.2882e-02,  4.3388e-02, -1.3379e-01,  8.6431e-02,\n",
       "                       -2.1441e-01,  1.0547e-01,  4.1019e-02,  6.8716e-02,  8.1876e-02,\n",
       "                       -1.0266e-01,  1.9188e-01,  1.0671e-01,  1.4689e-01,  2.9531e-02,\n",
       "                        1.4042e-01,  1.5049e-01,  1.0135e-01,  1.7034e-01, -4.0556e-02],\n",
       "                      [ 1.4703e-01,  5.1525e-02,  1.3455e-01,  2.1780e-01,  5.0629e-02,\n",
       "                       -8.2992e-02, -2.2248e-02,  6.2941e-02,  3.7701e-02, -6.1088e-02,\n",
       "                        1.7520e-01,  1.3435e-01, -1.3889e-01, -1.8224e-02, -2.1460e-02,\n",
       "                       -1.4014e-01,  3.2606e-02, -6.8645e-02, -1.1544e-01, -7.1902e-02],\n",
       "                      [-7.9724e-02,  2.1391e-01,  8.5772e-02,  2.2006e-01,  6.1310e-02,\n",
       "                        5.0634e-02,  9.1923e-02,  3.8372e-02, -1.7548e-01, -1.3223e-01,\n",
       "                       -5.4172e-02,  3.3624e-03,  8.4240e-02,  8.6097e-02,  1.7739e-01,\n",
       "                        4.5520e-02, -7.6320e-02, -1.6816e-01, -1.6377e-01, -1.3357e-01],\n",
       "                      [ 6.7439e-02,  1.2081e-01,  1.9563e-01, -7.0730e-02, -1.7638e-02,\n",
       "                       -6.6404e-02, -8.1712e-04,  7.1781e-02, -4.9127e-03,  1.0353e-02,\n",
       "                        2.8327e-02,  8.3275e-02, -6.7845e-02, -1.3333e-01,  5.2753e-02,\n",
       "                       -2.6476e-02, -1.0130e-01,  9.3906e-02,  2.0275e-03, -3.2526e-02],\n",
       "                      [-1.0824e-01, -2.9418e-02, -1.9014e-01,  5.0869e-02,  3.3550e-03,\n",
       "                        1.3569e-01,  1.4438e-02,  1.7876e-02,  6.0913e-02,  1.7015e-01,\n",
       "                       -1.0567e-01,  1.3349e-01,  5.4687e-02, -1.4917e-01,  7.7858e-02,\n",
       "                       -1.3410e-01, -7.0389e-02, -1.0659e-01, -1.5212e-01,  1.7611e-01],\n",
       "                      [-1.9986e-01,  1.1181e-01, -2.0125e-01, -2.7214e-02,  1.3336e-02,\n",
       "                        1.5897e-01, -8.0922e-02,  6.4358e-02,  3.4638e-02,  1.3157e-01,\n",
       "                        1.3295e-02,  8.3729e-02, -1.3508e-01, -1.5539e-01,  1.9925e-01,\n",
       "                       -1.6294e-01, -3.8069e-02, -2.7029e-02,  2.1866e-01, -4.8325e-02],\n",
       "                      [-1.7023e-01, -5.6682e-02,  1.1534e-01,  8.8614e-02,  1.8499e-01,\n",
       "                       -4.5775e-02,  1.0533e-01, -2.1043e-01,  8.6522e-02,  1.0274e-01,\n",
       "                        1.4425e-01, -1.2823e-01, -5.9696e-02,  5.4096e-02, -1.5890e-01,\n",
       "                       -1.7788e-01, -2.1512e-01,  1.2483e-02,  1.3886e-01,  5.7435e-02],\n",
       "                      [ 1.3630e-02,  7.7908e-02, -1.4052e-01,  2.3573e-02, -5.5548e-02,\n",
       "                       -1.1072e-01,  1.9283e-01, -1.7299e-01,  3.2770e-02, -1.9648e-01,\n",
       "                       -7.5881e-03, -9.9363e-02,  7.3597e-02, -1.2556e-01,  1.4311e-01,\n",
       "                       -1.8145e-01,  1.3431e-01, -1.6797e-01,  1.1976e-01, -9.4153e-02],\n",
       "                      [-5.2407e-04, -4.0813e-02,  1.4512e-01,  7.3280e-02, -1.9704e-01,\n",
       "                        2.0556e-02, -1.5426e-01,  1.6717e-01,  3.6739e-02, -5.7504e-02,\n",
       "                       -1.1408e-01,  7.7396e-02, -1.3392e-01,  1.0605e-01,  1.2201e-01,\n",
       "                        1.3725e-01,  5.8420e-02, -1.9574e-01,  1.6084e-01, -6.9367e-02],\n",
       "                      [-6.6103e-02, -2.4020e-02,  1.6349e-01,  1.7194e-01,  3.4525e-02,\n",
       "                        1.0670e-01, -2.0840e-01,  5.2061e-04, -1.9782e-01,  7.6101e-02,\n",
       "                       -1.5931e-01, -7.9693e-02,  1.5631e-01,  2.1370e-01, -1.3425e-01,\n",
       "                       -1.2410e-01,  2.7687e-02,  8.7901e-02, -3.6418e-02, -6.9832e-02]])),\n",
       "             ('hidden_stack.6.bias',\n",
       "              tensor([-0.1469,  0.0955,  0.2191, -0.1083,  0.1055,  0.1092,  0.1797, -0.0255,\n",
       "                       0.0070,  0.1643,  0.1413,  0.2209, -0.0641,  0.0885,  0.1561, -0.1518,\n",
       "                       0.0444, -0.0500,  0.1437, -0.1004])),\n",
       "             ('output_layer.weight',\n",
       "              tensor([[ 0.0157,  0.1338, -0.2048, -0.1888, -0.1145,  0.1434,  0.1499, -0.1527,\n",
       "                        0.0426, -0.1185, -0.1038,  0.1929,  0.1080, -0.0611,  0.0329,  0.0433,\n",
       "                        0.0524, -0.0765, -0.1317,  0.1480]])),\n",
       "             ('output_layer.bias', tensor([0.0301]))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_first_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b31cb931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7e09bc6f8350>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_count, loss_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_points = torch.tensor(np.linspace(), requires)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
